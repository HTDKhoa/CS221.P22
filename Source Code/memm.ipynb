{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11283390,"sourceType":"datasetVersion","datasetId":7054409},{"sourceId":12292790,"sourceType":"datasetVersion","datasetId":7367815}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import unicodedata as ud\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom tabulate import tabulate\nfrom time import time\nfrom math import ceil, floor\nfrom itertools import product\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport re\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:08:33.439791Z","iopub.execute_input":"2025-07-01T13:08:33.440030Z","iopub.status.idle":"2025-07-01T13:08:36.008013Z","shell.execute_reply.started":"2025-07-01T13:08:33.440005Z","shell.execute_reply":"2025-07-01T13:08:36.006521Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:08:36.011879Z","iopub.execute_input":"2025-07-01T13:08:36.012870Z","iopub.status.idle":"2025-07-01T13:08:36.019537Z","shell.execute_reply.started":"2025-07-01T13:08:36.012801Z","shell.execute_reply":"2025-07-01T13:08:36.018253Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = open('/kaggle/input/vn-data-pos/train.txt', encoding='utf-8').readlines()\nprint('Số lượng từ trong tập train_gold:', len(train))\ntrain[0:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:18.656005Z","iopub.execute_input":"2025-07-01T13:11:18.656391Z","iopub.status.idle":"2025-07-01T13:11:18.666325Z","shell.execute_reply.started":"2025-07-01T13:11:18.656360Z","shell.execute_reply":"2025-07-01T13:11:18.665427Z"}},"outputs":[{"name":"stdout","text":"Số lượng từ trong tập train_gold: 1225\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Vợ\\tN\\n', 'anh\\tN\\n', 'cũng\\tR\\n', 'thế\\tP\\n', 'nên\\tC\\n']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test = open('/kaggle/input/vn-data-pos/test.txt', encoding='utf-8').readlines()\nprint('Số lượng từ trong tập test_gold:', len(test))\ntest[0:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:34.933661Z","iopub.execute_input":"2025-07-01T13:11:34.934000Z","iopub.status.idle":"2025-07-01T13:11:34.946571Z","shell.execute_reply.started":"2025-07-01T13:11:34.933974Z","shell.execute_reply":"2025-07-01T13:11:34.945578Z"}},"outputs":[{"name":"stdout","text":"Số lượng từ trong tập test_gold: 162\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['Sau\\tE\\n', 'nhiều\\tA\\n', 'đêm\\tN\\n', 'suy_nghĩ\\tV\\n', ',\\tCH\\n']"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Prepare_Data","metadata":{}},{"cell_type":"code","source":"def build_vocab(gold_data, min_freq=2):\n    word_counts = defaultdict(int)\n    for line in gold_data:\n        line = line.strip()\n        if not line:\n            continue\n        if '\\t' in line:\n            word = line.split('\\t', 1)[0]\n        else:\n            parts = line.split()\n            if len(parts) >= 2:\n                word = parts[0]\n            else:\n                continue\n        word_counts[word] += 1\n\n    vocab = {word for word, count in word_counts.items() if count >= min_freq}\n    return vocab\n    \ndef prepare_data(gold_data, vocabs_dict=None, add_eos=False):\n    sentences = []\n    current_sentence = []\n    \n    sentence_endings = {'.', '!', '?', '...'}\n    \n    for line in gold_data:\n        line = line.strip()\n        if not line:  \n            if current_sentence:\n                if add_eos:\n                    current_sentence.append(('</s>', 'EOS'))\n                sentences.append(current_sentence)\n                current_sentence = []\n        else:\n            if '\\t' in line:\n                word, tag = line.split('\\t', 1)\n            else:\n                parts = line.split()\n                if len(parts) >= 2:\n                    word, tag = parts[0], parts[1]\n                else:\n                    continue\n                    \n            if vocabs_dict is not None and word not in vocabs_dict:\n                word = '--unk--'\n            \n            current_sentence.append((word, tag))\n            \n            if word in sentence_endings:\n                if current_sentence:\n                    if add_eos:\n                        current_sentence.append(('</s>', 'EOS'))\n                    sentences.append(current_sentence)\n                    current_sentence = []\n    \n    if current_sentence:\n        if add_eos:\n            current_sentence.append(('</s>', 'EOS'))\n        sentences.append(current_sentence)\n    \n    return sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:41.325853Z","iopub.execute_input":"2025-07-01T13:11:41.326221Z","iopub.status.idle":"2025-07-01T13:11:41.335448Z","shell.execute_reply.started":"2025-07-01T13:11:41.326190Z","shell.execute_reply":"2025-07-01T13:11:41.334279Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"vocab = build_vocab(train, min_freq=1)\ntrain_sentences = prepare_data(train, vocab, add_eos=False)\ntest_sentences = prepare_data(test, vocab, add_eos=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:49.386799Z","iopub.execute_input":"2025-07-01T13:11:49.387148Z","iopub.status.idle":"2025-07-01T13:11:49.393553Z","shell.execute_reply.started":"2025-07-01T13:11:49.387120Z","shell.execute_reply":"2025-07-01T13:11:49.392435Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# MEMM","metadata":{}},{"cell_type":"code","source":"BEGIN = '*'\nDEFAULT_CUTOFF_FRACT = 0.3333333\n\ndef vn_metaphone(word):\n    import unicodedata\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', word.lower())\n        if unicodedata.category(c) != 'Mn'\n    )\n\nclass Timer:\n    def __init__(self, name):\n        self.name = name\n        self.__start_time = None\n        self.__end_time = None\n        self.start()\n\n    def start(self):\n        self.__start_time = time()\n\n    def stop(self):\n        self.__end_time = time()\n        self.__get_elapsed__()\n\n    def __get_elapsed__(self):\n        elapsed = (self.__end_time - self.__start_time)\n        unit = \"seconds\"\n        if elapsed >= 3600:\n            unit = \"minutes\"\n            hours = elapsed / 3600\n            minutes = hours % 60\n            hours = floor(hours)\n            print(self.name, \"took\", str(hours), \"hours and\", \"{0:.2f}\".format(minutes), unit, \"to complete\")\n        elif elapsed >= 60:\n            minutes = floor(elapsed / 60)\n            seconds = elapsed % 60\n            print(self.name, \"took\", str(minutes), \"minutes and\", \"{0:.2f}\".format(seconds), unit, \"to complete\")\n        else:\n            print(self.name, \"took\", \"{0:.2f}\".format(elapsed), unit, \"to complete\")\n\nclass HistoryTuple:\n    def __init__(self, sequence_id, sentence, tags, index):\n        if index < 0 or index >= len(sentence):\n            raise IndexError\n        self.index = index\n        self.sequence_id = sequence_id\n        self.sentence = sentence\n        self.tags = tags\n        self.t2, self.t1 = self.__get_previous_tags__(tags)\n\n    def __get_previous_tags__(self, tags):\n        \"\"\"function to retrieve 2 previous tags for word in sentence\"\"\"\n        if len(self.tags) == 0:\n            return None, None\n        if self.index == 1:\n            return BEGIN, tags[self.index-1]\n        elif self.index == 0:\n            return BEGIN, BEGIN\n        else:\n            return tags[self.index-2], tags[self.index-1]\n\n    def getWord(self):\n        return self.sentence[self.index]\n\n    def getWordTag(self):\n        return self.tags[self.index]\n\n    def getT_2(self):\n        return self.t2\n\n    def getT_1(self):\n        return self.t1\n\n    def getIndex(self):\n        return self.index\n\n    def getTupleKey(self):\n        return self.t2, self.t1, self.sequence_id, self.index\n\n    def getPossibleTagSet(self, data, cutoff=None, add_common=False):\n        \"\"\"Return tag set which are possible for a given word\"\"\"\n        full_tag_set_size = data.getTagSetSize()\n        if cutoff is None:\n            cutoff = ceil(full_tag_set_size * DEFAULT_CUTOFF_FRACT)\n        elif cutoff >= full_tag_set_size:\n            return data.getTagSet()\n        \n        word = self.getWord()\n        tags_dict = data.getWordDict().get(word, False)\n        if tags_dict is False:\n            sorted_tags_list = data.getSortedTagsList()\n        else:\n            sorted_tags_list = sorted(tags_dict, key=tags_dict.get, reverse=True)\n        \n        if data.isNumberWord(word) and \"M\" not in sorted_tags_list[:cutoff]:\n            sorted_tags_list.insert(0, \"M\")\n        \n        remainder = cutoff - len(sorted_tags_list)\n        if remainder < 0:\n            return tuple(sorted_tags_list[:cutoff])\n        elif add_common is True and remainder > 0:\n            top_candidate_tags = data.getSortedTagsList()\n            sorted_tags_set = set(sorted_tags_list)\n            candidate_set = set(top_candidate_tags) - sorted_tags_set\n            while remainder > 0 and top_candidate_tags:\n                tag_candidate = top_candidate_tags.pop(0)\n                if tag_candidate in candidate_set:\n                    sorted_tags_list.append(tag_candidate)\n                    remainder -= 1\n        return tuple(sorted_tags_list)\n\nclass VietnameseDataReader:\n    def __init__(self, sentences, file_name=\"data\"):\n        self.file = file_name\n        self.sentences = []\n        self.tags = []\n        self.tag_dict = defaultdict(int)\n        self.word_dict = defaultdict(dict)\n        self.word_tag_dict = defaultdict(int)\n        self.numbers = 0\n        self.cap_no_start = 0\n        self.word_suffixes = {}\n        self.word_prefixes = {}\n        self.__process_sentences__(sentences)\n        self.__make_tuples__()\n        self.tags_bigrams, self.tags_trigrams = self.__tagsToNgrams__()\n        self.sorted_tags_list = sorted(self.tag_dict, key=self.tag_dict.get, reverse=True)\n\n    def __process_sentences__(self, sentences):\n        \"\"\"Process sentences in [(word, tag), ...] format\"\"\"\n        for sentence in sentences:\n            words = []\n            tags = []\n            for word, tag in sentence:\n                words.append(word)\n                tags.append(tag)\n                \n                # Update dictionaries\n                self.word_tag_dict[(word, tag)] += 1\n                self.tag_dict[tag] += 1\n                \n                if word not in self.word_dict:\n                    self.word_dict[word] = defaultdict(int)\n                self.word_dict[word][tag] += 1\n                \n                # Count numbers and capitals\n                if self.isNumberWord(word):\n                    self.numbers += 1\n                if word and word[0].isupper() and len(words) > 1:\n                    self.cap_no_start += 1\n            \n            self.sentences.append(tuple(words))\n            self.tags.append(tuple(tags))\n\n    def __make_tuples__(self):\n        self.sentences = tuple(self.sentences)\n        self.tags = tuple(self.tags)\n\n    def __tagsToNgrams__(self):\n        \"\"\"Create trigrams and bigrams from data\"\"\"\n        bigrams = defaultdict(int)\n        trigrams = defaultdict(int)\n        for tags in self.getTags():\n            tags = list(tags)\n            for i in range(2):\n                tags.insert(0, BEGIN)\n            for k in range(2, len(tags)):\n                trigrams[tuple(tags[k-2:k+1])] += 1\n                bigrams[tuple(tags[k-1:k+1])] += 1\n        return bigrams, trigrams\n\n    def __wordsToSuffixes__(self):\n        \"\"\"Create suffixes for all word,tag pairs\"\"\"\n        suffixes = defaultdict(int)\n        for word, tag in self.getWordTagDict():\n            for suffix in self.getSuffixesForWord(word):\n                suffixes[(suffix, tag)] += 1\n        return suffixes\n\n    def __wordsToPrefixes__(self):\n        \"\"\"Create prefixes for all word,tag pairs\"\"\"\n        prefixes = defaultdict(int)\n        for word, tag in self.getWordTagDict():\n            for prefix in self.getPrefixesForWord(word):\n                prefixes[(prefix, tag)] += 1\n        return prefixes\n\n    def getSuffixesForWord(self, word):\n        \"\"\"Generate suffixes for a given word\"\"\"\n        suffixes = self.word_suffixes.get(word, False)\n        if suffixes is not False:\n            return suffixes\n        suffixes = []\n        if word.isalpha():\n            boundary = min(5, len(word))\n            for i in range(1, boundary):\n                suffixes.append(word[-i:])\n        suffixes = tuple(suffixes)\n        self.word_suffixes[word] = suffixes\n        return suffixes\n\n    def getPrefixesForWord(self, word):\n        \"\"\"Generate prefixes for a given word\"\"\"\n        prefixes = self.word_prefixes.get(word, False)\n        if prefixes is not False:\n            return prefixes\n        prefixes = []\n        if word.isalpha():\n            boundary = min(5, len(word))\n            for i in range(2, boundary):\n                prefixes.append(word[:i])\n        prefixes = tuple(prefixes)\n        self.word_prefixes[word] = prefixes\n        return prefixes\n\n    @staticmethod\n    def isNumberWord(word):\n        if word.isdigit():\n            return True\n        elif word.isnumeric():\n            return True\n        elif word.isdecimal():\n            return True\n        else:\n            for char in ('-', ',', '.', '\\/'):\n                word = word.replace(char, '')\n                if word.isdigit():\n                    return True\n            return False\n\n    def getTagSet(self):\n        return tuple(self.tag_dict.keys())\n\n    def getTagSetSize(self):\n        return len(self.tag_dict)\n\n    def getWordDict(self):\n        return self.word_dict\n\n    def getTagDict(self):\n        return self.tag_dict\n\n    def getTagDictSize(self):\n        return len(self.tag_dict)\n\n    def getWordDictSize(self):\n        return len(self.word_dict)\n\n    def getSentences(self):\n        return self.sentences\n\n    def getTags(self):\n        return self.tags\n\n    def getSentencesSize(self):\n        return len(self.sentences)\n\n    def getTagsSize(self):\n        return len(self.tags)\n\n    def getSentenceByIndex(self, index):\n        return self.sentences[index]\n\n    def getTagsByIndex(self, index):\n        return self.tags[index]\n\n    def getWordTagDict(self):\n        return self.word_tag_dict\n\n    def getSortedTagsList(self):\n        return self.sorted_tags_list.copy()\n\n    def getTopNTagsForWord(self, word, n):\n        tags_dict = self.getWordDict().get(word, False)\n        if tags_dict is False:\n            return self.getTopNTags(n)\n        sorted_tags = sorted(tags_dict, key=tags_dict.get, reverse=True)\n        if n == 1:\n            return sorted_tags[0] if sorted_tags else self.sorted_tags_list[0]\n        elif n >= len(sorted_tags):\n            return sorted_tags\n        else:\n            return sorted_tags[:n]\n\n    def getTopNTags(self, n):\n        if n == 1:\n            return self.sorted_tags_list[0]\n        elif n >= len(self.sorted_tags_list):\n            return self.sorted_tags_list\n        else:\n            return self.sorted_tags_list[:n]\n\n    def getNumbers(self):\n        return self.numbers\n\n    def getCapNoStart(self):\n        return self.cap_no_start\n\n    def getCapStart(self):\n        return self.getSentencesSize()\n\nclass VietnameseFeaturesFactory:\n    def __init__(self, data, cutoff=0):\n        self.data = data\n        self.type = \"vietnamese_optimized\"\n        self._cutoff = cutoff\n        self._features_index = {}\n        self.histories_dict = {}\n        self.null_histories_set = set()\n        \n        # Từ đơn âm tiết hay đa âm tiết\n        self.syllable_info = self.__analyze_syllables__()\n        \n        self.__generateFeaturesIndex__()\n\n    def __analyze_syllables__(self):\n        syllable_info = {}\n        for sentence in self.data.getSentences():\n            for word in sentence:\n                # Đếm số âm tiết bằng cách đếm số dấu gạch dưới + 1\n                if '_' in word:\n                    # Từ có dấu gạch dưới: \"bóng_đá\" = 2 âm tiết, \"học_sinh_viên\" = 3 âm tiết\n                    syllable_count = word.count('_') + 1\n                else:\n                    # Từ đơn âm tiết: \"tôi\", \"đẹp\", \"nhà\" = 1 âm tiết\n                    syllable_count = 1\n                syllable_info[word] = syllable_count\n        return syllable_info\n\n    def getSyllablePrefixes(self, word):\n        \"\"\"Return all syllable-level prefixes for a word (joined by '_'). E.g. 'học_sinh_viên' -> ['học', 'học_sinh']\"\"\"\n        if '_' not in word:\n            return []\n        sylls = word.split('_')\n        return ['_'.join(sylls[:i]) for i in range(1, len(sylls))]\n    \n    def getSyllableSuffixes(self, word):\n        \"\"\"Return all syllable-level suffixes for a word (joined by '_'). E.g. 'học_sinh_viên' -> ['viên', 'sinh_viên']\"\"\"\n        if '_' not in word:\n            return []\n        sylls = word.split('_')\n        return ['_'.join(sylls[i:]) for i in range(1, len(sylls))]\n\n    def getFeaturesIndices(self, tag, history, in_data=True):\n        \"\"\"Lấy feature indices tối ưu cho tiếng Việt\"\"\"\n        indices = []\n        word = history.getWord()\n        position = history.getIndex()\n        sentence = history.sentence\n        \n        # 1. f100: (Word,Tag) pair\n        if in_data:\n            feature_idx = self._features_index.get((\"f100\", (word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n\n        2 # Phonetic (Metaphone) feature\n        metaphone = vn_metaphone(word)\n        feature_idx = self._features_index.get((\"fMetaphone\", (metaphone, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 3. f103: Trigram Tags\n        feature_idx = self._features_index.get((\"f103\", (history.getT_2(), history.getT_1(), tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 4. Bigram Tags\n        feature_idx = self._features_index.get((\"f104\", (history.getT_1(), tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 5. Window features: W_{i-2}, W_{i-1}, W_{i+1}, W_{i+2}\n        for offset in [-2, -1, 1, 2]:\n            idx = position + offset\n            if 0 <= idx < len(sentence):\n                context_word = sentence[idx]\n                feature_idx = self._features_index.get((\"fWindow\", (offset, context_word, tag)), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n        \n        # 6. Word pair features: (W_{i-1}, W_i), (W_i, W_{i+1})\n        if position > 0:\n            prev_word = sentence[position - 1]\n            feature_idx = self._features_index.get((\"fWordPair\", (prev_word, word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        if position < len(sentence) - 1:\n            next_word = sentence[position + 1]\n            feature_idx = self._features_index.get((\"fWordPair\", (word, next_word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 7. Punctuation feature\n        if tag == 'CH':\n            feature_idx = self._features_index.get((\"fPunct\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 8. Number feature\n        if tag == 'M':\n            feature_idx = self._features_index.get((\"fNum\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 9. Quantifier feature\n        if tag == 'L':\n            feature_idx = self._features_index.get((\"fQuantifier\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 10. Capitalization features\n        if word and word[0].isupper():\n            if position == 0:\n                feature_idx = self._features_index.get((\"fCapStart\", tag), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n            else:\n                feature_idx = self._features_index.get((\"fCapNoStart\", tag), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n        \n        # 11. Syllable count feature\n        syllable_count = self.syllable_info.get(word, 1)\n        feature_idx = self._features_index.get((\"fSyllable\", (syllable_count, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 12. Word length feature\n        length_category = \"short\" if len(word) <= 3 else \"medium\" if len(word) <= 5 else \"long\"\n        feature_idx = self._features_index.get((\"fLength\", (length_category, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n\n        # 13. SyllablePrefix\n        for prefix in self.getSyllablePrefixes(word):\n            feature_idx = self._features_index.get((\"fSyllablePrefix\", (prefix, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 14. SyllableSuffix\n        for suffix in self.getSyllableSuffixes(word):\n            feature_idx = self._features_index.get((\"fSyllableSuffix\", (suffix, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 15. Position in sentence features\n        sent_len = len(sentence)\n        if position == 0:\n            pos_feature = \"first\"\n        elif position == sent_len - 1:\n            pos_feature = \"last\"\n        elif position / sent_len < 0.3:\n            pos_feature = \"early\"\n        elif position / sent_len > 0.7:\n            pos_feature = \"late\"\n        else:\n            pos_feature = \"middle\"\n        \n        feature_idx = self._features_index.get((\"fPosition\", (pos_feature, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        return indices\n\n    def __generateFeaturesIndex__(self):\n        \"\"\"Generate features index cho tất cả features\"\"\"\n        feature_names = [\n            \"f100\", \"f103\", \"f104\", \"fWindow\", \"fWordPair\", \"fPunct\", \n            \"fNum\", \"fQuantifier\", \"fCapStart\", \"fCapNoStart\", \n            \"fSyllable\", \"fLength\", \"fPosition\",\n            \"fSyllablePrefix\", \"fSyllableSuffix\", \"fMetaphone\"\n        ]\n        \n        # Build feature dictionaries\n        feature_dicts = {}\n        \n        # Existing features\n        feature_dicts[\"f100\"] = self.data.getWordTagDict()\n        feature_dicts[\"f103\"] = self.data.tags_trigrams\n        feature_dicts[\"f104\"] = self.data.tags_bigrams\n        \n        # New features\n        feature_dicts.update(self.__build_new_feature_dicts__())\n        \n        # Generate indices\n        keys = []\n        for name in feature_names:\n            if name in feature_dicts:\n                features = []\n                for feature in feature_dicts[name].keys():\n                    if feature_dicts[name].get(feature) > self._cutoff:\n                        features.append((name, feature))\n                keys.extend(features)\n        \n        for i, key in enumerate(keys):\n            self._features_index[key] = i\n        \n        self.features_list = tuple(keys)\n        self._features_vector_length = len(keys)\n\n    def __build_new_feature_dicts__(self):\n        new_dicts = defaultdict(lambda: defaultdict(int))\n        \n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            \n            for i, (word, tag) in enumerate(zip(sentence, tags)):\n                # Window features\n                for offset in [-2, -1, 1, 2]:\n                    idx = i + offset\n                    if 0 <= idx < len(sentence):\n                        context_word = sentence[idx]\n                        new_dicts[\"fWindow\"][(offset, context_word, tag)] += 1\n\n                metaphone = vn_metaphone(word)\n                new_dicts[\"fMetaphone\"][(metaphone, tag)] += 1\n                \n                # Word pair features\n                if i > 0:\n                    prev_word = sentence[i - 1]\n                    new_dicts[\"fWordPair\"][(prev_word, word, tag)] += 1\n                \n                if i < len(sentence) - 1:\n                    next_word = sentence[i + 1]\n                    new_dicts[\"fWordPair\"][(word, next_word, tag)] += 1\n                \n                # Punctuation feature\n                if tag == 'CH':\n                    new_dicts[\"fPunct\"][tag] += 1\n                \n                # Number feature\n                if tag == 'M':\n                    new_dicts[\"fNum\"][tag] += 1\n                \n                # Quantifier feature\n                if tag == 'L':\n                    new_dicts[\"fQuantifier\"][tag] += 1\n                    \n                # fSyllablePrefix\n                for prefix in self.getSyllablePrefixes(word):\n                    new_dicts[\"fSyllablePrefix\"][(prefix, tag)] += 1\n                \n                # fSyllableSuffix\n                for suffix in self.getSyllableSuffixes(word):\n                    new_dicts[\"fSyllableSuffix\"][(suffix, tag)] += 1\n                \n                # Syllable count\n                syllable_count = self.syllable_info.get(word, 1)\n                new_dicts[\"fSyllable\"][(syllable_count, tag)] += 1\n                \n                # Word length\n                length_category = \"short\" if len(word) <= 3 else \"medium\" if len(word) <= 5 else \"long\"\n                new_dicts[\"fLength\"][(length_category, tag)] += 1\n                \n                # Position in sentence\n                sent_len = len(sentence)\n                if i == 0:\n                    pos_feature = \"first\"\n                elif i == sent_len - 1:\n                    pos_feature = \"last\"\n                elif i / sent_len < 0.3:\n                    pos_feature = \"early\"\n                elif i / sent_len > 0.7:\n                    pos_feature = \"late\"\n                else:\n                    pos_feature = \"middle\"\n                \n                new_dicts[\"fPosition\"][(pos_feature, tag)] += 1\n                \n                # Capitalization features\n                if word and word[0].isupper():\n                    if i == 0:\n                        new_dicts[\"fCapStart\"][tag] += 1\n                    else:\n                        new_dicts[\"fCapNoStart\"][tag] += 1\n        \n        return dict(new_dicts)\n\n    def getFeaturesVectorLength(self):\n        return self._features_vector_length\n    \n    def getCutoffParameter(self):\n        return self._cutoff\n    \n    def getEmpiricalCounts(self):\n        \"\"\"Get empirical counts vector\"\"\"\n        empirical_counts = np.zeros(self.getFeaturesVectorLength(), dtype=float)\n        \n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            \n            for i in range(len(sentence)):\n                history = HistoryTuple(k, sentence, tags, i)\n                features = self.getFeaturesIndices(tags[i], history, True)\n                for feature_idx in features:\n                    empirical_counts[feature_idx] += 1.0\n        \n        return empirical_counts\n\n\nclass ViterbiAlgorithm:\n    \"\"\"Viterbi Algorithm for Vietnamese POS tagging\"\"\"\n    def __init__(self, sequence_id, sentence, sentence_tags, model, cutoff=None):\n        self.sequence_id = sequence_id\n        self.sentence = sentence\n        self.sentence_tags = sentence_tags\n        self.data = model.data\n        if cutoff is None:\n            self.cutoff = self.data.getTagSetSize()\n        else:\n            self.cutoff = cutoff\n        self.tags_set = self.data.getTagSet()\n        self.prob_func = model.probability\n        self.weights = model.getWeights()\n        self.pi = {(-1, BEGIN, BEGIN): 1.0}\n        self.bp = {}\n        self.tag_sequence = []\n\n    def run(self):\n        \"\"\"Main Viterbi algorithm\"\"\"\n        sentence_length = len(self.sentence)\n        for k in range(sentence_length):\n            tag_pairs = tuple(product(self.__calc_possible_tags_set__(k-1), \n                                    self.__calc_possible_tags_set__(k)))\n            for u, v in tag_pairs:\n                key = (k, u, v)\n                self.pi[key], self.bp[k] = self.__calc_max_probability__(key)\n                if self.pi[key] == 0.0000:\n                    self.bp[k] = self.data.getTopNTagsForWord(self.sentence[k], 1)\n        \n        self.bp[sentence_length], self.bp[sentence_length+1] = self.__calc_last_tags__(sentence_length)\n        for k in range(sentence_length):\n            self.tag_sequence.append(self.bp.get(k+2, False))\n\n    def __calc_possible_tags_set__(self, index):\n        \"\"\"Return possible tag set for a given position in the sentence\"\"\"\n        if index < 0:\n            return (BEGIN,)\n        return HistoryTuple(self.sequence_id, self.sentence, self.sentence_tags, index).getPossibleTagSet(self.data, self.cutoff, add_common=False)\n\n    def __calc_max_probability__(self, key):\n        \"\"\"Calculate maximum probability for each iteration\"\"\"\n        k = key[0]\n        u = key[1]\n        v = key[2]\n        if k < 0:\n            return 1.0, BEGIN\n        max_pi = 0.00000\n        max_bp = None\n        possible_tags_set = self.__calc_possible_tags_set__(k-2)\n        for t in possible_tags_set:\n            new_key = (k-1, t, u)\n            history = HistoryTuple(self.sequence_id, self.sentence, self.sentence_tags, k)\n            pi_value = self.pi.get(new_key, 0.00000) * self.prob_func(v, history, self.weights)\n            if pi_value >= max_pi:\n                max_pi = pi_value\n                max_bp = t\n        return max_pi, max_bp\n\n    def __calc_last_tags__(self, sentence_length):\n        \"\"\"Return last 2 tags in the sequence\"\"\"\n        max_pi = 0.0\n        max_bp = ()\n        tag_pairs = tuple(product(self.__calc_possible_tags_set__(sentence_length-2), \n                                self.__calc_possible_tags_set__(sentence_length-1)))\n        for u, v in tag_pairs:\n            key = (sentence_length-1, u, v)\n            pi_value = self.pi.get(key, None)\n            if pi_value and pi_value >= max_pi:\n                max_pi = pi_value\n                max_bp = (u, v)\n        return max_bp\n\n    def getBestTagSequence(self):\n        return tuple(self.tag_sequence)\n\nclass MEMM:\n    \"\"\"MEMM model for Vietnamese POS tagging\"\"\"\n    def __init__(self, feature_factory, regularizer=0, pretrained_weights=None):\n        self.data = feature_factory.data\n        self.feature_factory = feature_factory\n        self.regularizer = float(regularizer)\n        self.cache = self.getTrainedWeightsCacheName()\n        self.weights = self.__initializeWeights__(pretrained_weights)\n        self.train_results = None\n        self.predictions = {}\n        self.correct_tags = defaultdict(int)\n        self.wrong_tags = defaultdict(int)\n        self.wrong_tag_pairs = defaultdict(int)\n        self.wrong_tags_dicts = {}\n\n    def __initializeWeights__(self, pretrained_weights):\n        \"\"\"Initialize model weights\"\"\"\n        weights_vector_length = self.feature_factory.getFeaturesVectorLength()\n        weights = np.zeros(weights_vector_length, dtype=float)\n        if pretrained_weights is True:\n            weights = self.loadTrainedWeights(self.getTrainedWeightsCacheName())\n        elif pretrained_weights is not None and type(pretrained_weights) is np.ndarray and len(pretrained_weights) == weights_vector_length:\n            weights = pretrained_weights\n        return weights\n\n    def getWeights(self):\n        return self.weights\n\n    def getFeatures(self, tag, history, in_data=False):\n        \"\"\"Get feature instances indices for given tag and HistoryTuple\"\"\"\n        history_key = (tag, history.getTupleKey())\n        if history_key in self.feature_factory.null_histories_set:\n            return []\n        feature = self.feature_factory.histories_dict.get(history_key, None)\n        if feature is None:\n            feature = self.feature_factory.getFeaturesIndices(tag, history, in_data)\n            if len(feature) == 0:\n                self.feature_factory.null_histories_set.add(history_key)\n        return feature\n\n    def calc_dot_product(self, features, weights):\n        \"\"\"Calculate dot product between feature and weights vectors\"\"\"\n        total = 0.0\n        for index in features:\n            total += weights[index]\n        return total\n\n    def calcDenominatorBatch(self, history, weights, cutoff=None):\n        \"\"\"Calculate sum in denominator of probability calculation\"\"\"\n        full_tag_set_size = self.data.getTagSetSize()\n        tag_set = history.getPossibleTagSet(self.data, cutoff, add_common=True)\n        remainder = float(full_tag_set_size) - len(tag_set)\n        total = 0.0\n        for tag in tag_set:\n            features = self.getFeatures(tag, history, False)\n            if len(features) == 0:\n                temp = 1.0\n            else:\n                temp = np.exp(self.calc_dot_product(features, weights), dtype=float)\n            total += temp\n        if remainder > 0:\n            total += remainder\n        if total == 0.0:\n            total = 0.0001\n        return total\n\n    def calcNominator(self, features, weights):\n        \"\"\"Calculate nominator in probability calculation\"\"\"\n        if len(features) == 0:\n            nominator = 1.0\n        else:\n            product = self.calc_dot_product(features, weights)\n            if product == 0.0:\n                nominator = 1.0\n            else:\n                nominator = np.exp(product, dtype=float)\n        return nominator\n\n    def probability(self, tag, history, weights, features=None):\n        \"\"\"Calculate probability of specific tag given history\"\"\"\n        if features is None:\n            features = self.getFeatures(tag, history, True)\n        nominator = self.calcNominator(features, weights)\n        denominator = self.calcDenominatorBatch(history, weights)\n        return float(nominator/denominator)\n\n    def calc_loss(self, weights):\n        \"\"\"Calculate loss function value over entire dataset\"\"\"\n        timer = Timer(\"Loss Calculation\")\n        features_sum = 0.0\n        denominators_sum = 0.0\n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            for i in range(len(sentence)):\n                history = HistoryTuple(k, sentence, tags, i)\n                features_sum += self.calc_dot_product(self.getFeatures(tags[i], history, True), weights)\n                denominators_sum += np.log(self.calcDenominatorBatch(history, weights, self.data.getTagSetSize()), dtype=float)\n        if self.regularizer == 1.0:\n            regularization_sum = np.sum(np.power(weights, 2, dtype=float), dtype=float) / 2.0\n        elif self.regularizer != 0.0:\n            regularization_sum = self.regularizer * np.sum(np.power(weights, 2, dtype=float), dtype=float) / 2.0\n        else:\n            regularization_sum = 0.0\n        total = regularization_sum + denominators_sum - features_sum\n        timer.stop()\n        print(\"Loss:\", total)\n        return total\n\n    def calcExpectedCountsDict(self, weights):\n        \"\"\"Calculate expected counts\"\"\"\n        dictionary = defaultdict(float)\n        \n        # Process all sentences sequentially\n        for i in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(i)\n            tags = self.data.getTagsByIndex(i)\n            for j in range(len(sentence)):\n                history = HistoryTuple(i, sentence, tags, j)\n                self.calcExpectedCountsBatchInternal(history, weights, dictionary)\n        \n        return Counter(dictionary)\n\n    def calcExpectedCountsBatchInternal(self, history, weights, dictionary):\n        \"\"\"Internal function for expected counts calculation\"\"\"\n        cutoff = self.data.getTagSetSize()\n        tag_set = history.getPossibleTagSet(self.data, cutoff, add_common=True)\n        for tag in tag_set:\n            features = self.getFeatures(tag, history, False)\n            if len(features) == 0:\n                continue\n            probability = self.probability(tag, history, weights, features)\n            for index in features:\n                dictionary[index] += probability\n\n    def calc_gradient(self, weights):\n        \"\"\"Calculate gradient vector over entire dataset\"\"\"\n        timer = Timer(\"Gradient Calculation\")\n        empirical_counts = self.feature_factory.getEmpiricalCounts()\n        expected_counts_dict = self.calcExpectedCountsDict(weights)\n        expected_counts = self.calcExpectedCountsVector(expected_counts_dict)\n        if self.regularizer == 1.0:\n            regularization_counts = weights\n        elif self.regularizer != 0.0:\n            regularization_counts = self.regularizer * weights\n        else:\n            regularization_counts = 0.0\n        total = regularization_counts + expected_counts - empirical_counts\n        timer.stop()\n        print(\"Average Gradient value:\", np.mean(total))\n        return total\n\n    def calcExpectedCountsVector(self, dictionary):\n        \"\"\"Convert ExpectedCounts dictionary to numpy vector\"\"\"\n        indexes = dictionary.keys()\n        vector = np.zeros(self.feature_factory.getFeaturesVectorLength(), dtype=float)\n        for index in indexes:\n            vector[index] = dictionary.get(index, 0.0)\n        return vector\n\n    def fit(self, max_iter=100, tolerance=0.001, factr=1e12, save=True):\n        \"\"\"Train the model using L-BFGS-B\"\"\"\n        timer = Timer(\"Training\")\n        weights, loss, result = fmin_l_bfgs_b(self.calc_loss, self.weights, self.calc_gradient, pgtol=tolerance, maxiter=max_iter, factr=factr)\n        if result.get(\"warnflag\", False) != 0:\n            print(\"Warning - gradient didn't converge within\", max_iter, \"iterations\")\n        result['loss'] = loss\n        print(result)\n        self.train_results = result\n        self.weights = weights\n        timer.stop()\n        if save:\n            import os\n            os.makedirs('./cache', exist_ok=True)\n            with open(self.getTrainedWeightsCacheName(), 'wb') as cache:\n                pickle.dump({'weights': self.weights, 'train_results': self.train_results}, cache)\n\n    def predictSequential(self, data, cutoff):\n        \"\"\"Run predictions sequentially instead of using threads\"\"\"\n        timer = Timer(\"Predicting \" + str(data.getSentencesSize()) + \" sentences\")\n        predictions = {}\n        \n        for i in range(data.getSentencesSize()):\n            sentence = data.getSentenceByIndex(i)\n            tags = data.getTagsByIndex(i)\n            viterbi = ViterbiAlgorithm(i, sentence, tags, self, cutoff)\n            viterbi.run()\n            predictions[i] = viterbi.getBestTagSequence()\n        \n        timer.stop()\n        return predictions\n\n    def predict(self, data, cutoff=3):\n        \"\"\"Predict tags for entire dataset\"\"\"\n        timer = Timer(\"Inference\")\n        self.predictions[data.file] = self.predictSequential(data, cutoff)\n        timer.stop()\n\n    def evaluate(self, data, verbose=False):\n        \"\"\"Evaluate model predictions vs truth\"\"\"\n        assert data.getTagsSize() == len(self.predictions.get(data.file, [])), \"Predictions and truth are not the same length!\"\n        timer = Timer(\"Evaluation\")\n        accuracies = []\n        for i in range(data.getTagsSize()):\n            truth = data.getTagsByIndex(i)\n            prediction = self.predictions.get(data.file).get(i, False)\n            accuracies.append(self.accuracy(truth, prediction, verbose))\n        avg = np.mean(accuracies)\n        minimum = np.min(accuracies)\n        maximum = np.max(accuracies)\n        med = np.median(accuracies)\n        print(\"Results for\", data.file)\n        print(\"Total Average Accuracy:\", avg)\n        print(\"Minimal Accuracy:\", minimum)\n        print(\"Maximal Accuracy:\", maximum)\n        print(\"Median Accuracy:\", med)\n        self.confusionTable(data.file)\n        self.confusionMatrix(data.file)\n        timer.stop()\n        return data.file, avg, minimum, maximum, med\n\n    def accuracy(self, truth, predictions, verbose=False):\n        \"\"\"Calculate accuracy for a given sentence\"\"\"\n        assert len(truth) == len(predictions), \"Predictions and truth are not the same length!\"\n        correct = 0\n        for i in range(len(truth)):\n            key = truth[i]\n            subkey = predictions[i]\n            if truth[i] == predictions[i]:\n                correct += 1\n                self.correct_tags[key] += 1\n            else:\n                self.wrong_tags[key] += 1\n                self.wrong_tag_pairs[(key, subkey)] += 1\n                if self.wrong_tags_dicts.get(key, False) is False:\n                    self.wrong_tags_dicts[key] = defaultdict(int)\n                self.wrong_tags_dicts[key][subkey] += 1\n                if verbose:\n                    print(\"Mistake in index\", i, \"(truth, prediction): \", key, subkey)\n        result = float(correct) / len(truth)\n        if verbose:\n            print(\"Accuracy:\", result)\n        return result\n\n    def confusionMatrix(self, file, n=10):\n        \"\"\"Produce Confusion Matrix for top n wrong tags\"\"\"\n        top_wrong_tags = sorted(self.wrong_tags, key=self.wrong_tags.get, reverse=True)[:n]\n        header = top_wrong_tags\n        rows = []\n        for truth in top_wrong_tags:\n            columns = [truth]\n            for prediction in top_wrong_tags:\n                if truth == prediction:\n                    columns.append(self.correct_tags.get(truth))\n                else:\n                    columns.append(self.wrong_tag_pairs.get((truth, prediction)))\n            rows.append(columns)\n        print(\"Confusion Matrix for \" + self.feature_factory.type + \" model on \" + file + \" dataset\")\n        header.insert(0, \"Truth \\ Predicted\")\n        print(tabulate(rows, headers=header))\n\n    def confusionTable(self, file, n=10):\n        \"\"\"Produce Confusion Table for top n wrong tags\"\"\"\n        top_wrong_tags = sorted(self.wrong_tag_pairs, key=self.wrong_tag_pairs.get, reverse=True)[:n]\n        header = (\"Correct Tag\", \"Model's Tag\", \"Frequency\")\n        rows = []\n        for truth, prediction in tuple(top_wrong_tags):\n            freq = self.wrong_tag_pairs.get((truth, prediction))\n            rows.append((truth, prediction, freq))\n        print(\"Confusion Table for \" + self.feature_factory.type + \" model on \" + file + \" dataset\")\n        print(tabulate(rows, headers=header))\n\n    def getTrainedWeightsCacheName(self):\n        \"\"\"Get cache file name according to model parameters\"\"\"\n        prefix = \"./cache/\"\n        parameters = \"data-\" + str(self.data.getSentencesSize()) + \"_features-\" + self.feature_factory.type +\"_weightSize-\"\\\n                     + str(self.feature_factory.getFeaturesVectorLength()) + \"_cutoff-\" + str(self.feature_factory.getCutoffParameter()) \\\n                     + \"_regularizer-\" + str(self.regularizer)\n        suffix = \"_trained_weights.pkl\"\n        return prefix + parameters + suffix\n\n    def loadTrainedWeights(self, file):\n        \"\"\"Load pretrained weights from cache file\"\"\"\n        with open(file, 'rb') as cache:\n            trained = pickle.load(cache)\n            weights = trained.get('weights')\n        return weights\n\n    def evaluate_with_sklearn(self, data, verbose=False):\n        \"\"\"Evaluate model using sklearn metrics\"\"\"\n        assert data.getTagsSize() == len(self.predictions.get(data.file, [])), \"Predictions and truth are not the same length!\"\n        \n        # Flatten all true tags and predictions\n        y_true = []\n        y_pred = []\n        \n        for i in range(data.getTagsSize()):\n            truth = data.getTagsByIndex(i)\n            prediction = self.predictions.get(data.file).get(i, [])\n            \n            y_true.extend(truth)\n            y_pred.extend(prediction)\n        \n        overall_accuracy = accuracy_score(y_true, y_pred)\n        \n        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n        \n        df_report = pd.DataFrame(report).transpose()\n        \n        # Print results\n        print(f\"\\n=== Sklearn Evaluation Results for MEMM ===\")\n        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n        print(\"\\nDetailed Classification Report:\")\n        print(df_report.round(4))\n        \n        # Create a summary table for main metrics\n        main_tags = [tag for tag in df_report.index if tag not in ['accuracy', 'macro avg', 'weighted avg']]\n        summary_data = []\n        \n        for tag in main_tags:\n            summary_data.append({\n                'Tag': tag,\n                'Precision': df_report.loc[tag, 'precision'],\n                'Recall': df_report.loc[tag, 'recall'],\n                'F1-Score': df_report.loc[tag, 'f1-score'],\n                'Support': int(df_report.loc[tag, 'support'])\n            })\n        \n        summary_df = pd.DataFrame(summary_data)\n        \n        # Add overall metrics\n        overall_metrics = pd.DataFrame([\n            {\n                'Tag': 'macro avg',\n                'Precision': df_report.loc['macro avg', 'precision'],\n                'Recall': df_report.loc['macro avg', 'recall'],\n                'F1-Score': df_report.loc['macro avg', 'f1-score'],\n                'Support': int(df_report.loc['macro avg', 'support'])\n            },\n            {\n                'Tag': 'weighted avg',\n                'Precision': df_report.loc['weighted avg', 'precision'],\n                'Recall': df_report.loc['weighted avg', 'recall'],\n                'F1-Score': df_report.loc['weighted avg', 'f1-score'],\n                'Support': int(df_report.loc['weighted avg', 'support'])\n            }\n        ])\n        \n        final_summary = pd.concat([summary_df, overall_metrics], ignore_index=True)\n        \n        print(\"\\n=== Summary Table ===\")\n        print(final_summary.round(4).to_string(index=False))\n        \n        # Show confusion matrix for top tags\n        if verbose:\n            from sklearn.metrics import confusion_matrix\n            import seaborn as sns\n            import matplotlib.pyplot as plt\n            \n            # Get top 10 most frequent tags\n            tag_counts = pd.Series(y_true).value_counts()\n            top_tags = tag_counts.head(10).index.tolist()\n            \n            y_true_filtered = []\n            y_pred_filtered = []\n            \n            for true_tag, pred_tag in zip(y_true, y_pred):\n                if true_tag in top_tags and pred_tag in top_tags:\n                    y_true_filtered.append(true_tag)\n                    y_pred_filtered.append(pred_tag)\n            \n            if len(y_true_filtered) > 0 and len(y_pred_filtered) > 0:\n                cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_tags)\n                \n                plt.figure(figsize=(12, 10))\n                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                           xticklabels=top_tags, yticklabels=top_tags)\n                plt.title('Confusion Matrix - Top 10 Tags')\n                plt.ylabel('True Label')\n                plt.xlabel('Predicted Label')\n                plt.show()\n            else:\n                print(\"Not enough data for confusion matrix visualization\")\n        \n        return {\n            'overall_accuracy': overall_accuracy,\n            'classification_report': report,\n            'summary_df': final_summary,\n            'y_true': y_true,\n            'y_pred': y_pred\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:12:22.377965Z","iopub.execute_input":"2025-07-01T13:12:22.378377Z","iopub.status.idle":"2025-07-01T13:12:22.556786Z","shell.execute_reply.started":"2025-07-01T13:12:22.378346Z","shell.execute_reply":"2025-07-01T13:12:22.555745Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"=== Training Vietnamese MEMM ===\")\n\ntrain_data = VietnameseDataReader(train_sentences, \"train\")\ntest_data = VietnameseDataReader(test_sentences, \"test\")\n\nprint(f\"Train data: {train_data.getSentencesSize()} sentences\")\nprint(f\"Test data: {test_data.getSentencesSize()} sentences\")\nprint(f\"Tag set: {train_data.getTagSet()}\")\n\nfeature_factory = VietnameseFeaturesFactory(train_data, cutoff=3)\nprint(f\"Features vector length: {feature_factory.getFeaturesVectorLength()}\")\n\nmemm = MEMM(feature_factory, regularizer=0.01)\n\nprint(\"Training MEMM...\")\nmemm.fit(max_iter=50, tolerance=0.01, save=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:12:22.635684Z","iopub.execute_input":"2025-07-01T13:12:22.636037Z","iopub.status.idle":"2025-07-01T13:14:13.549739Z","shell.execute_reply.started":"2025-07-01T13:12:22.636009Z","shell.execute_reply":"2025-07-01T13:14:13.548522Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"=== Training Vietnamese MEMM ===\nTrain data: 61 sentences\nTest data: 10 sentences\nTag set: ('N', 'R', 'P', 'C', 'V', 'T', 'CH', 'A', 'M', 'Nc', 'E', 'L', 'Cc', 'Nu', 'Np', 'Ny')\nFeatures vector length: 589\nTraining MEMM...\nLoss Calculation took 0.27 seconds to complete\nLoss: 3396.42118474363\nGradient Calculation took 1.66 seconds to complete\nAverage Gradient value: -7.016235144312394\nLoss Calculation took 0.24 seconds to complete\nLoss: 2992.815848676373\nGradient Calculation took 1.73 seconds to complete\nAverage Gradient value: -6.780345359532868\nLoss Calculation took 0.24 seconds to complete\nLoss: 2183.319329051375\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -2.6985957323143785\nLoss Calculation took 0.24 seconds to complete\nLoss: 1799.7243041368065\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -2.4669691420375086\nLoss Calculation took 0.24 seconds to complete\nLoss: 1456.0111601490516\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: -2.136130775536988\nLoss Calculation took 0.24 seconds to complete\nLoss: 908.5591396461432\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: -0.7928198250804244\nLoss Calculation took 0.25 seconds to complete\nLoss: 16.524758500931966\nGradient Calculation took 1.73 seconds to complete\nAverage Gradient value: 2.468151220762477\nLoss Calculation took 0.24 seconds to complete\nLoss: -732.5779844531753\nGradient Calculation took 1.75 seconds to complete\nAverage Gradient value: 5.937776866188613\nLoss Calculation took 0.25 seconds to complete\nLoss: -1380.8237488959558\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: 1.7826264511022223\nLoss Calculation took 0.24 seconds to complete\nLoss: -2616.4655579276505\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -0.3742553969882154\nLoss Calculation took 0.24 seconds to complete\nLoss: -6876.031208844599\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: -0.47027343469557437\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37711056188\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 3002.694868809666\nLoss Calculation took 0.24 seconds to complete\nLoss: -6995.334990304864\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -0.5212075020911069\nLoss Calculation took 0.24 seconds to complete\nLoss: -24624.45190051269\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 78.15717458051375\nLoss Calculation took 0.24 seconds to complete\nLoss: -30286.508555499662\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 909.6246352499945\nLoss Calculation took 0.24 seconds to complete\nLoss: -32235.496095429007\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 2134.9893245852495\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37711056188\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.694868809666\nLoss Calculation took 0.27 seconds to complete\nLoss: -8235.1822258021\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -1.8078745876031197\nLoss Calculation took 0.24 seconds to complete\nLoss: -25000.402061018325\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 76.89998920112227\nLoss Calculation took 0.24 seconds to complete\nLoss: -30406.173603507756\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: 901.9859587963829\nLoss Calculation took 0.24 seconds to complete\nLoss: -32268.053845006667\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 2127.877281757702\nLoss Calculation took 0.24 seconds to complete\nLoss: -32835.18483126002\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 2767.0737228603593\nLoss Calculation took 0.24 seconds to complete\nLoss: -32973.151176257816\nGradient Calculation took 1.75 seconds to complete\nAverage Gradient value: 2949.9136288956793\nLoss Calculation took 0.24 seconds to complete\nLoss: -33003.28257275409\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 2991.439269280186\nLoss Calculation took 0.26 seconds to complete\nLoss: -33009.67236080374\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: 3000.3208396774503\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.01849871088\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 3002.195309823349\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.301691312714\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 3002.5898001700234\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.361250043665\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: 3002.672772775659\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.373775162516\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.690222094404\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37640914319\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 3002.6938916269132\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37696305688\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.69466331284\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37707954221\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 3002.694825594667\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37710403866\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 3002.694859721771\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37710919013\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.6948668985224\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37711027349\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.6948684077697\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.377110501184\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.6948687251547\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.37711054903\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 3002.6948687918893\nLoss Calculation took 0.24 seconds to complete\nLoss: 2237896316.041295\nGradient Calculation took 1.73 seconds to complete\nAverage Gradient value: 3.6049346909282683e+18\nLoss Calculation took 0.24 seconds to complete\nLoss: 766220497.7032547\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 9900358.722061241\nLoss Calculation took 0.24 seconds to complete\nLoss: 94550651.628293\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 459.2044998315908\nLoss Calculation took 0.24 seconds to complete\nLoss: 11914396.927907612\nGradient Calculation took 1.78 seconds to complete\nAverage Gradient value: 551.5473087345764\nLoss Calculation took 0.24 seconds to complete\nLoss: 1779043.2515574854\nGradient Calculation took 1.70 seconds to complete\nAverage Gradient value: 371.9658458684406\nLoss Calculation took 0.24 seconds to complete\nLoss: 329180.0280758529\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 260.499170297362\nLoss Calculation took 0.24 seconds to complete\nLoss: 59979.2749251308\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 7.50052706444335\nLoss Calculation took 0.24 seconds to complete\nLoss: -5894.368721457016\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: -1.3658798019707235\nLoss Calculation took 0.24 seconds to complete\nLoss: -24593.96312639639\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 1.2707103952085412\nLoss Calculation took 0.24 seconds to complete\nLoss: -30338.000271063902\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 5.0162939403531\nLoss Calculation took 0.24 seconds to complete\nLoss: -32209.514896247303\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 7.429598093495512\nLoss Calculation took 0.24 seconds to complete\nLoss: -32799.960980947406\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 39.0181509451038\nLoss Calculation took 0.24 seconds to complete\nLoss: -32955.05390914517\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 218.0981039356482\nLoss Calculation took 0.24 seconds to complete\nLoss: -32995.35010571666\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 872.9514698717088\nLoss Calculation took 0.24 seconds to complete\nLoss: -33006.91488985765\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 2007.6536509599184\nLoss Calculation took 0.24 seconds to complete\nLoss: -33010.304155470505\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 2713.0190131105173\nLoss Calculation took 0.25 seconds to complete\nLoss: -33011.14191379974\nGradient Calculation took 1.72 seconds to complete\nAverage Gradient value: 2935.8847841377446\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.32699178199\nGradient Calculation took 1.68 seconds to complete\nAverage Gradient value: 2988.2943138891314\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.366500558244\nGradient Calculation took 1.69 seconds to complete\nAverage Gradient value: 2999.638828241344\nLoss Calculation took 0.24 seconds to complete\nLoss: -33011.374867638246\nGradient Calculation took 1.77 seconds to complete\nAverage Gradient value: 3002.0484949304187\nWarning - gradient didn't converge within 50 iterations\n{'grad': array([-2.03596754e+01, -9.69508354e+00, -5.33229595e+01, -7.46521433e+01,\n       -5.81705013e+00, -5.81705013e+00, -1.26036086e+01, -3.87803342e+00,\n       -3.39327924e+01, -1.06645919e+01, -3.87803342e+00, -7.75606683e+00,\n       -4.84754177e+00, -9.69508354e+00, -5.81705013e+00, -3.87803342e+00,\n       -3.87803342e+00, -4.84754177e+00, -9.69508354e+00, -3.87803342e+00,\n       -1.45426253e+01, -1.64816420e+01, -1.16341003e+01, -6.78655848e+00,\n       -3.87803342e+00, -7.75606683e+00, -8.72557519e+00, -3.87803342e+00,\n       -3.87803342e+00, -1.35731170e+01, -9.69508354e+00, -9.69508354e+00,\n       -2.03596754e+01, -8.72557519e+00, -3.87803342e+00, -7.75606683e+00,\n       -4.84754177e+00, -4.84754177e+00, -8.72557519e+00, -1.35731170e+01,\n       -7.75606683e+00, -3.87803342e+00, -5.81705013e+00, -5.81705013e+00,\n       -4.84754177e+00, -3.87803342e+00, -3.87803342e+00, -4.84754177e+00,\n       -3.87803342e+00, -6.78655848e+00, -3.87803342e+00, -4.84754177e+00,\n       -7.75606683e+00, -3.87803342e+00, -4.84754177e+00, -3.87803342e+00,\n       -3.87803342e+00, -4.00128311e+00, -8.05383444e-01,  1.45884244e+00,\n       -9.51771498e-01,  6.22036590e-01,  4.06492084e+00, -1.02569716e+00,\n       -3.94123615e+00,  4.22544795e-01,  6.80425056e-02, -3.93540639e+00,\n        6.84170756e+00,  4.60398870e-02, -2.56390534e-02,  1.21412271e+00,\n        1.04492019e+00,  4.73862276e+00, -6.80832275e-03, -4.64975753e+00,\n        9.74595193e-02,  1.41250499e-02, -4.35436182e+00,  1.87295367e+00,\n        7.35500482e-01,  3.61977037e-01,  1.91300147e+00, -4.57250722e+00,\n        5.91960030e+00, -4.71496883e+00, -9.82758031e-01,  2.80636546e+00,\n       -2.42110701e+00,  6.80482239e-01, -3.21475713e-01,  1.63792446e+00,\n       -3.10204131e+00, -1.91379683e+00,  5.30459472e+04, -1.98466635e+00,\n       -5.95394162e+00,  9.87232472e-02,  3.48588925e+00,  1.10297188e+00,\n        5.31533232e+00, -7.62099617e-02,  6.96106024e-01,  8.62699403e-01,\n        2.04919627e+00, -2.86048982e+00, -2.29752815e+00,  3.17820465e-02,\n       -2.76004576e+00,  2.49936394e-01,  1.05041033e+00,  6.91558460e-02,\n       -6.10747417e+00,  1.49855037e+00,  2.73033518e+00, -2.80115247e+00,\n        7.37056772e-01, -2.47167385e-02,  9.60170671e-02, -3.94791065e+00,\n       -5.98871648e+00,  6.24968001e-02,  1.03802883e+00, -9.56674132e-01,\n       -1.32587640e+00, -4.41957856e+00,  1.94433748e-02, -3.99562150e+00,\n        2.44174220e-02,  1.22446179e-02,  2.05083012e+00, -3.02828006e+00,\n        2.76066166e+00, -8.01220330e-01, -1.72684547e+00,  1.72318679e+04,\n        2.64728437e-02,  7.34205392e-02, -2.98687777e+00,  1.86979836e+00,\n        1.35809413e-02, -9.49088282e-01, -1.75403117e+00,  6.42433417e-02,\n        7.90712087e-03, -4.00128311e+00, -1.43782251e+01, -9.10001328e+00,\n       -3.50999605e+00, -3.87136802e+00,  1.70444889e+01, -3.98268273e+00,\n        1.45884244e+00, -1.49873445e+01,  3.37506258e-01, -2.07332092e+00,\n        9.32172473e+02,  1.15957561e-01, -1.95934325e+00, -4.00117180e+00,\n       -2.97071175e+00,  1.69690690e-01,  3.24975250e+01,  1.33587411e+00,\n       -5.29884595e+00,  6.57715826e+00, -9.24602977e-01,  8.62597847e+00,\n        4.20088717e+00, -6.23815633e+00,  1.95020882e+01,  2.90416418e-02,\n       -2.56390534e-02,  1.12148457e+01, -1.28510267e+01,  1.24787289e+02,\n       -9.32614248e+00, -6.95401739e+00,  1.48488747e+02, -2.43866516e+00,\n        5.30428842e+04, -4.14384775e+00, -5.08332764e+00, -1.98472279e+00,\n        1.87295367e+00, -4.97286011e+00,  3.34168213e+00, -2.03785449e+00,\n       -3.01648349e+00,  2.26408096e+00, -1.17011901e+00,  7.69728615e+00,\n       -3.96953560e+00, -5.95394162e+00, -5.54359402e-02, -1.67844110e+00,\n        3.59473191e+00, -1.58352077e+00, -1.86575135e+00,  8.90951941e+02,\n        2.98111345e+05, -4.17849051e+00, -3.01810635e+00, -2.99806820e-03,\n       -5.99260204e-01, -9.45298541e-01, -4.42119825e+00,  7.55159317e-03,\n       -3.78821355e+00,  1.22446179e-02,  1.46112022e-01, -4.25247423e-01,\n        3.23607473e-01, -1.74316777e+00, -3.80975787e+00,  1.86979836e+00,\n       -2.97582551e+00, -1.84109867e+00, -5.19078939e+00, -1.88917398e+00,\n       -2.12718458e+00, -1.01933257e+00,  9.04481253e+00,  2.65883434e+01,\n       -9.18452954e-01, -1.91115195e+00, -2.77626145e+00, -2.77671134e+00,\n       -1.85825487e+00, -2.45470232e+00,  6.89742297e+02,  2.81819708e+05,\n       -1.05697109e+00,  1.99311985e+00,  1.43933445e+00,  8.76097451e+01,\n        2.46130804e+02,  2.08274105e+00,  6.36017414e+00, -1.50686565e+00,\n        8.53748824e+00,  4.76876890e-01, -2.01651257e+00, -3.20200644e+00,\n        5.30546846e+04,  3.31732736e-02,  1.22607956e+00,  5.09213241e+00,\n        2.38118792e+00,  1.68635355e+00,  1.36833117e-01,  1.14492204e-01,\n       -3.12043407e+00,  8.61261407e+00, -1.01506525e+00,  9.84378389e-01,\n       -7.86528150e-01,  2.74007791e+00,  2.03630118e+00, -2.41296080e+00,\n        6.19228157e+00, -9.39728059e-01, -1.43973745e+00, -1.61182836e+00,\n        1.17757800e-01, -4.07665513e-01, -1.64613535e+00,  2.12469605e+00,\n        1.21082904e-01, -1.93455817e+00,  2.25535737e-01,  1.10609414e+00,\n        5.98586182e-02,  2.02857847e+00, -7.50373446e-01, -3.43774594e+00,\n       -9.18962649e-01,  4.03384494e+00, -9.48758285e-01,  5.84962377e-02,\n       -6.87452600e-01,  1.96096493e+00,  4.24246339e+00,  4.87779343e-01,\n        1.18859087e+00,  9.40024704e-02, -9.43241554e-01,  3.95902194e+00,\n       -1.56374701e+00,  1.68855901e+00,  1.08347640e+00, -8.71885802e-02,\n        4.98401009e+00,  1.06860110e+00,  3.14718478e+00,  5.24334792e-02,\n        3.85124587e-01,  3.31257507e+00,  1.17728295e-01,  5.30466931e+04,\n       -9.63236289e-01,  1.93753157e+00,  6.37979993e+00, -8.41426441e-01,\n        4.51264263e-01,  2.59696498e-02,  8.90966959e+02, -2.13566715e-01,\n        4.94017943e-02, -1.83062271e+00,  4.97327041e+00,  2.84969209e-02,\n        3.96777677e-02, -1.83627616e+00,  4.71044026e-02,  1.51120626e-02,\n        6.69022524e-02,  2.70788789e+00, -7.09437828e-02, -2.27739068e-01,\n       -3.98647163e+00, -1.36696932e-01,  6.92259557e-02, -9.25619189e-01,\n        1.35067626e+00,  1.08159925e+00,  8.83757909e-02,  4.41936245e+02,\n        2.16649763e-02,  1.93086767e-02,  1.38803694e-01,  1.68333106e-01,\n        5.65603616e-02,  9.39107805e-02,  8.01545361e-02,  2.77017441e-02,\n        2.97543658e-01, -3.98692362e+00,  4.26535661e-02, -1.00183034e+00,\n       -1.02421064e+00,  9.79778877e-02,  1.03210451e+00, -6.85722797e-01,\n        6.66265463e-02, -3.93841823e+00,  2.57098009e-01, -6.75290034e-03,\n       -2.59692298e+00,  5.09804761e-02, -2.05294963e+00, -1.50628381e+00,\n        9.46710869e-01, -1.22349187e+00,  3.33430137e-01,  7.77152901e-02,\n        8.77016634e-02,  6.70707952e-02,  1.02884409e+00, -1.32728766e+00,\n        5.43550363e-02, -3.88762697e+00,  5.44591351e-02,  2.74644997e-02,\n       -1.75492861e+00, -2.95168786e+00,  1.09425616e-01, -3.04999020e+00,\n        1.05154640e-01,  9.42767119e-02, -2.89454294e+00, -9.75431032e-01,\n        3.43799995e-01,  1.80021409e-02,  7.15184366e-02,  1.06519374e+00,\n        9.34560633e-02,  3.39356475e-02,  1.34517880e-01, -1.45769332e-02,\n       -9.57543705e-01,  7.39598847e-02,  9.13938039e-02,  4.80243705e-02,\n        8.10735307e-02,  7.19670784e-02,  1.26167390e-01,  8.91781332e-02,\n        7.70939859e-02,  5.28019656e-02,  1.51961843e-01,  3.08826074e-02,\n        9.05338849e-02,  1.14113656e-01,  3.85918914e-02,  7.52332397e-02,\n        9.27467881e-02,  9.02349443e-02,  7.91401176e-03,  3.44505167e-02,\n        7.28636960e-02,  9.23132788e-02,  2.75082558e-02,  9.23094472e-02,\n       -1.92590232e+00, -9.33613736e-01, -1.73582704e+01, -2.22783044e+01,\n       -1.07154404e+01, -3.98827717e+00,  1.46121088e+00, -3.82221426e-03,\n        2.84668935e+00, -5.93838890e+00,  1.68447069e-02,  1.88188745e+00,\n        1.62476573e+01, -3.59638589e+01, -3.54372874e+01, -1.82648106e+01,\n        8.94306636e+02,  5.07044469e+01, -9.06147518e+00, -1.71622460e+01,\n       -3.61619740e+01,  2.30113189e+01, -2.21465669e+01,  2.61846957e+02,\n       -2.74696395e+01,  9.91210000e+00, -9.60076854e+00,  1.35003917e+01,\n       -3.96593191e+00,  2.99452863e+05,  1.02343483e+00, -6.60037151e+00,\n        2.19550743e+01, -1.34108746e-03, -1.89505802e+01, -1.02909262e+01,\n       -1.60700438e+01, -2.10409148e+01,  1.77303849e+01, -2.41067797e+01,\n       -7.01028698e+00, -1.58640417e+01, -2.00987117e+01,  3.19762517e+01,\n       -3.61606797e+01, -4.02006461e+00,  2.71201868e+01, -1.42033309e+01,\n        2.70265395e+02, -3.28338027e+01,  6.21578796e+00,  8.85803520e+00,\n       -4.04505187e+00,  1.34790423e+01, -2.85676585e+00,  2.81926967e+05,\n        9.15345779e+02, -8.64053139e+00,  1.75258587e+04, -5.55700172e+00,\n       -7.94330566e+00, -9.52117030e+00, -4.00128311e+00, -8.04665035e+00,\n       -4.78883990e+00, -7.39283661e+00,  1.19457173e+01,  1.11286810e+01,\n       -4.80778882e+00, -1.73370831e+01, -1.45092851e+00,  1.45884244e+00,\n       -5.39794567e+00,  3.19841902e+01, -3.24689920e+00, -3.63500290e+01,\n       -5.28858421e+00, -7.17741837e+00, -1.09416980e+01,  1.37919391e+03,\n        3.26287753e+01,  8.89352088e+02, -5.72839331e+00,  9.26112562e+00,\n       -1.94254066e+01,  5.30487446e+04, -2.56390534e-02,  1.61981689e+02,\n       -8.37093504e+00, -8.12630691e-01, -6.06111306e+00,  1.87295367e+00,\n       -1.85160702e+00, -1.04562618e+00, -5.83062057e+00, -5.24946516e+00,\n       -5.95394162e+00, -6.76266246e+00,  9.04230779e+01,  2.98073913e+05,\n       -8.48212129e+00, -5.03262300e+00,  1.22446179e-02, -3.02296609e+00,\n        1.86979836e+00, -3.84200560e+00,  2.25278062e+01, -3.25593398e+00,\n        5.30340869e-02,  2.41509220e-01, -1.80060536e+00,  4.88757883e-02,\n        1.04764894e+00,  5.82803894e-02,  7.48838946e-02,  1.53121368e-01,\n        8.57649892e-02,  5.30340869e-02,  1.28773550e+00,  6.35860519e-02,\n        5.73815614e-02, -1.19087323e+00, -2.07909219e-01,  6.77006181e-02,\n       -9.13531371e-01,  1.53121368e-01,  1.04642671e-01, -9.37035638e-02,\n        1.40370152e+00,  1.18372426e+00,  4.71539720e-01, -1.01905373e+00,\n       -1.89560220e+00, -9.47257425e-01, -5.39720350e+00, -1.81763444e+00,\n       -2.31443890e+00,  3.71729686e+00,  6.07281463e+00,  5.30340869e-02,\n       -5.89222707e+00, -1.91362118e+00,  7.30410660e-02, -4.60870818e-01,\n       -2.79558391e+00,  1.20813179e+00, -3.63562116e+00, -3.70091951e+00,\n        4.55088172e-01, -4.10473446e+00, -2.25811404e+00, -1.94151539e+00,\n        9.17513673e+02, -2.83146977e+00, -3.24195855e+00,  3.55093145e-02,\n        1.27236581e+00, -1.76768817e+00,  1.18107319e+00,  4.03525852e-01,\n       -1.03614725e+00, -8.97558268e-01,  1.00973421e-01, -4.86772643e+00,\n        6.35860519e-02, -3.14761503e-01, -5.10322152e+00,  1.27479780e-01,\n       -3.30104442e+00,  9.10552885e-02, -3.13269670e-01,  6.87299302e-02,\n        7.91281529e-02, -1.58464855e+00,  7.48838946e-02, -1.86622930e+00,\n        1.40677490e-01, -9.26221637e-01,  7.44556302e-02,  1.53121368e-01,\n       -1.94597492e+00,  5.60688769e-02,  8.57649892e-02, -3.89551064e+00,\n       -3.86017340e+00]), 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 57, 'nit': 11, 'warnflag': 2, 'loss': -33011.374867638246}\nTraining took 1 minutes and 50.89 seconds to complete\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Make predictions\nprint(\"Making predictions...\")\nmemm.predict(test_data, cutoff=3)\n\nprint(\"Evaluating with original method...\")\nresults = memm.evaluate(test_data, verbose=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Evaluating with sklearn...\")\nsklearn_results = memm.evaluate_with_sklearn(test_data, verbose=True)\n\ncomparison_data = {\n    'Method': ['Original MEMM', 'Sklearn'],\n    'Accuracy': [results[1], sklearn_results['overall_accuracy']],\n    'Features': [f\"{feature_factory.getFeaturesVectorLength()} features\", \"Same model\"]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(\"\\n=== Method Comparison ===\")\nprint(comparison_df.to_string(index=False))\n\n# Save results to CSV\nsklearn_results['summary_df'].to_csv('pos_tagging_results.csv', index=False)\nprint(\"Results saved to 'pos_tagging_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:14:13.550958Z","iopub.execute_input":"2025-07-01T13:14:13.551300Z","iopub.status.idle":"2025-07-01T13:14:14.815633Z","shell.execute_reply.started":"2025-07-01T13:14:13.551260Z","shell.execute_reply":"2025-07-01T13:14:14.814718Z"}},"outputs":[{"name":"stdout","text":"Making predictions...\nPredicting 10 sentences took 0.07 seconds to complete\nInference took 0.07 seconds to complete\nEvaluating with original method...\nResults for test\nTotal Average Accuracy: 0.8107142857142857\nMinimal Accuracy: 0.6666666666666666\nMaximal Accuracy: 0.9090909090909091\nMedian Accuracy: 0.8257575757575758\nConfusion Table for vietnamese_optimized model on test dataset\nCorrect Tag    Model's Tag      Frequency\n-------------  -------------  -----------\nA              V                        7\nV              N                        5\nR              V                        3\nN              V                        3\nV              CH                       3\nE              C                        2\nA              N                        2\nE              V                        2\nC              T                        1\nV              E                        1\nConfusion Matrix for vietnamese_optimized model on test dataset\nTruth \\ Predicted      V    A    R    E    N    C    T    P\n-------------------  ---  ---  ---  ---  ---  ---  ---  ---\nV                     29         1    1    5\nA                      7    3              2\nR                      3        12         1\nE                      2              9         2\nN                      3                  22\nC                                               1    1\nT                           1\nP                                          1             20\nEvaluation took 0.01 seconds to complete\n\n==================================================\nEvaluating with sklearn...\n\n=== Sklearn Evaluation Results for MEMM ===\nOverall Accuracy: 0.7901\n\nDetailed Classification Report:\n              precision  recall  f1-score   support\nA                0.7500  0.2308    0.3529   13.0000\nC                0.3333  0.5000    0.4000    2.0000\nCH               0.8333  1.0000    0.9091   20.0000\nCc               1.0000  1.0000    1.0000    3.0000\nE                0.9000  0.6923    0.7826   13.0000\nL                1.0000  1.0000    1.0000    1.0000\nM                1.0000  1.0000    1.0000    6.0000\nN                0.7097  0.8800    0.7857   25.0000\nNc               1.0000  1.0000    1.0000    2.0000\nP                1.0000  0.9524    0.9756   21.0000\nR                0.9231  0.7500    0.8276   16.0000\nT                0.0000  0.0000    0.0000    1.0000\nV                0.6591  0.7436    0.6988   39.0000\naccuracy         0.7901  0.7901    0.7901    0.7901\nmacro avg        0.7776  0.7499    0.7486  162.0000\nweighted avg     0.8025  0.7901    0.7801  162.0000\n\n=== Summary Table ===\n         Tag  Precision  Recall  F1-Score  Support\n           A     0.7500  0.2308    0.3529       13\n           C     0.3333  0.5000    0.4000        2\n          CH     0.8333  1.0000    0.9091       20\n          Cc     1.0000  1.0000    1.0000        3\n           E     0.9000  0.6923    0.7826       13\n           L     1.0000  1.0000    1.0000        1\n           M     1.0000  1.0000    1.0000        6\n           N     0.7097  0.8800    0.7857       25\n          Nc     1.0000  1.0000    1.0000        2\n           P     1.0000  0.9524    0.9756       21\n           R     0.9231  0.7500    0.8276       16\n           T     0.0000  0.0000    0.0000        1\n           V     0.6591  0.7436    0.6988       39\n   macro avg     0.7776  0.7499    0.7486      162\nweighted avg     0.8025  0.7901    0.7801      162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n=== Method Comparison ===\n       Method  Accuracy     Features\nOriginal MEMM  0.810714 589 features\n      Sklearn  0.790123   Same model\nResults saved to 'pos_tagging_results.csv'\n","output_type":"stream"}],"execution_count":13}]}