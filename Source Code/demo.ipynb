{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11283390,"sourceType":"datasetVersion","datasetId":7054409},{"sourceId":12292790,"sourceType":"datasetVersion","datasetId":7367815},{"sourceId":455944,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":369120,"modelId":389993}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:18:04.328114Z","iopub.execute_input":"2025-07-02T03:18:04.328463Z","iopub.status.idle":"2025-07-02T03:18:04.334450Z","shell.execute_reply.started":"2025-07-02T03:18:04.328429Z","shell.execute_reply":"2025-07-02T03:18:04.333407Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install py_vncorenlp\n!pip install streamlit\n!pip install streamlit localtunnel\n!pip install imutils\n!npm install -g localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:18:04.335446Z","iopub.execute_input":"2025-07-02T03:18:04.335796Z","iopub.status.idle":"2025-07-02T03:18:28.175465Z","shell.execute_reply.started":"2025-07-02T03:18:04.335762Z","shell.execute_reply":"2025-07-02T03:18:28.174207Z"}},"outputs":[{"name":"stdout","text":"Collecting py_vncorenlp\n  Downloading py_vncorenlp-0.1.4.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting pyjnius (from py_vncorenlp)\n  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: py_vncorenlp\n  Building wheel for py_vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for py_vncorenlp: filename=py_vncorenlp-0.1.4-py3-none-any.whl size=4305 sha256=f6b5756177a34125e3b49315d043cdc4427f0b39b0f12b7a7568e307f57d90d6\n  Stored in directory: /root/.cache/pip/wheels/d5/d9/bf/62632cdb007c702a0664091e92a0bb1f18a2fcecbe962d9827\nSuccessfully built py_vncorenlp\nInstalling collected packages: pyjnius, py_vncorenlp\nSuccessfully installed py_vncorenlp-0.1.4 pyjnius-1.6.1\nCollecting streamlit\n  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.46.1\nRequirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.46.1)\n\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\nadded 22 packages in 3s\n\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\n\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.4.2\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.4.2\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.4.2\u001b[24m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile app_temp.py\n\nimport streamlit as st\nimport pickle\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom collections import defaultdict, Counter\nfrom tabulate import tabulate\nfrom time import time\nfrom math import ceil, floor\nfrom itertools import product\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport pandas as pd\nimport py_vncorenlp\n\nBEGIN = '*'\nDEFAULT_CUTOFF_FRACT = 0.3333333\n\n@st.cache_resource\ndef load_vncorenlp_global():\n    \"\"\"Load VnCoreNLP once globally and cache it\"\"\"\n    try:\n        # Download model if not exists\n        py_vncorenlp.download_model(save_dir='/kaggle/working/')\n        segmenter = py_vncorenlp.VnCoreNLP(save_dir='/kaggle/working/')\n        return segmenter\n    except Exception as e:\n        st.error(f\"Error loading VnCoreNLP: {str(e)}\")\n        return None\n\n# Initialize VnCoreNLP globally at module level\nGLOBAL_SEGMENTER = None\n\ndef get_global_segmenter():\n    \"\"\"Get the global segmenter instance\"\"\"\n    global GLOBAL_SEGMENTER\n    if GLOBAL_SEGMENTER is None:\n        GLOBAL_SEGMENTER = load_vncorenlp_global()\n    return GLOBAL_SEGMENTER\n\ndef vn_metaphone(word):\n    import unicodedata\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', word.lower())\n        if unicodedata.category(c) != 'Mn'\n    )\n\nclass Timer:\n    def __init__(self, name):\n        self.name = name\n        self.__start_time = None\n        self.__end_time = None\n        self.start()\n\n    def start(self):\n        self.__start_time = time()\n\n    def stop(self):\n        self.__end_time = time()\n        self.__get_elapsed__()\n\n    def __get_elapsed__(self):\n        elapsed = (self.__end_time - self.__start_time)\n        unit = \"seconds\"\n        if elapsed >= 3600:\n            unit = \"minutes\"\n            hours = elapsed / 3600\n            minutes = hours % 60\n            hours = floor(hours)\n            print(self.name, \"took\", str(hours), \"hours and\", \"{0:.2f}\".format(minutes), unit, \"to complete\")\n        elif elapsed >= 60:\n            minutes = floor(elapsed / 60)\n            seconds = elapsed % 60\n            print(self.name, \"took\", str(minutes), \"minutes and\", \"{0:.2f}\".format(seconds), unit, \"to complete\")\n        else:\n            print(self.name, \"took\", \"{0:.2f}\".format(elapsed), unit, \"to complete\")\n\nclass HistoryTuple:\n    def __init__(self, sequence_id, sentence, tags, index):\n        if index < 0 or index >= len(sentence):\n            raise IndexError\n        self.index = index\n        self.sequence_id = sequence_id\n        self.sentence = sentence\n        self.tags = tags\n        self.t2, self.t1 = self.__get_previous_tags__(tags)\n\n    def __get_previous_tags__(self, tags):\n        \"\"\"function to retrieve 2 previous tags for word in sentence\"\"\"\n        if len(self.tags) == 0:\n            return None, None\n        if self.index == 1:\n            return BEGIN, tags[self.index-1]\n        elif self.index == 0:\n            return BEGIN, BEGIN\n        else:\n            return tags[self.index-2], tags[self.index-1]\n\n    def getWord(self):\n        return self.sentence[self.index]\n\n    def getWordTag(self):\n        return self.tags[self.index]\n\n    def getT_2(self):\n        return self.t2\n\n    def getT_1(self):\n        return self.t1\n\n    def getIndex(self):\n        return self.index\n\n    def getTupleKey(self):\n        return self.t2, self.t1, self.sequence_id, self.index\n\n    def getPossibleTagSet(self, data, cutoff=None, add_common=False):\n        \"\"\"Return tag set which are possible for a given word\"\"\"\n        full_tag_set_size = data.getTagSetSize()\n        if cutoff is None:\n            cutoff = ceil(full_tag_set_size * DEFAULT_CUTOFF_FRACT)\n        elif cutoff >= full_tag_set_size:\n            return data.getTagSet()\n        \n        word = self.getWord()\n        tags_dict = data.getWordDict().get(word, False)\n        if tags_dict is False:\n            sorted_tags_list = data.getSortedTagsList()\n        else:\n            sorted_tags_list = sorted(tags_dict, key=tags_dict.get, reverse=True)\n        \n        if data.isNumberWord(word) and \"M\" not in sorted_tags_list[:cutoff]:\n            sorted_tags_list.insert(0, \"M\")\n        \n        remainder = cutoff - len(sorted_tags_list)\n        if remainder < 0:\n            return tuple(sorted_tags_list[:cutoff])\n        elif add_common is True and remainder > 0:\n            top_candidate_tags = data.getSortedTagsList()\n            sorted_tags_set = set(sorted_tags_list)\n            candidate_set = set(top_candidate_tags) - sorted_tags_set\n            while remainder > 0 and top_candidate_tags:\n                tag_candidate = top_candidate_tags.pop(0)\n                if tag_candidate in candidate_set:\n                    sorted_tags_list.append(tag_candidate)\n                    remainder -= 1\n        return tuple(sorted_tags_list)\n\nclass VietnameseDataReader:\n    def __init__(self, sentences, file_name=\"data\"):\n        self.file = file_name\n        self.sentences = []\n        self.tags = []\n        self.tag_dict = defaultdict(int)\n        self.word_dict = defaultdict(dict)\n        self.word_tag_dict = defaultdict(int)\n        self.numbers = 0\n        self.cap_no_start = 0\n        self.word_suffixes = {}\n        self.word_prefixes = {}\n        self.__process_sentences__(sentences)\n        self.__make_tuples__()\n        self.tags_bigrams, self.tags_trigrams = self.__tagsToNgrams__()\n        self.sorted_tags_list = sorted(self.tag_dict, key=self.tag_dict.get, reverse=True)\n\n    def __process_sentences__(self, sentences):\n        \"\"\"Process sentences in [(word, tag), ...] format\"\"\"\n        for sentence in sentences:\n            words = []\n            tags = []\n            for word, tag in sentence:\n                words.append(word)\n                tags.append(tag)\n                \n                # Update dictionaries\n                self.word_tag_dict[(word, tag)] += 1\n                self.tag_dict[tag] += 1\n                \n                if word not in self.word_dict:\n                    self.word_dict[word] = defaultdict(int)\n                self.word_dict[word][tag] += 1\n                \n                # Count numbers and capitals\n                if self.isNumberWord(word):\n                    self.numbers += 1\n                if word and word[0].isupper() and len(words) > 1:\n                    self.cap_no_start += 1\n            \n            self.sentences.append(tuple(words))\n            self.tags.append(tuple(tags))\n\n    def __make_tuples__(self):\n        self.sentences = tuple(self.sentences)\n        self.tags = tuple(self.tags)\n\n    def __tagsToNgrams__(self):\n        \"\"\"Create trigrams and bigrams from data\"\"\"\n        bigrams = defaultdict(int)\n        trigrams = defaultdict(int)\n        for tags in self.getTags():\n            tags = list(tags)\n            for i in range(2):\n                tags.insert(0, BEGIN)\n            for k in range(2, len(tags)):\n                trigrams[tuple(tags[k-2:k+1])] += 1\n                bigrams[tuple(tags[k-1:k+1])] += 1\n        return bigrams, trigrams\n\n    def __wordsToSuffixes__(self):\n        \"\"\"Create suffixes for all word,tag pairs\"\"\"\n        suffixes = defaultdict(int)\n        for word, tag in self.getWordTagDict():\n            for suffix in self.getSuffixesForWord(word):\n                suffixes[(suffix, tag)] += 1\n        return suffixes\n\n    def __wordsToPrefixes__(self):\n        \"\"\"Create prefixes for all word,tag pairs\"\"\"\n        prefixes = defaultdict(int)\n        for word, tag in self.getWordTagDict():\n            for prefix in self.getPrefixesForWord(word):\n                prefixes[(prefix, tag)] += 1\n        return prefixes\n\n    def getSuffixesForWord(self, word):\n        \"\"\"Generate suffixes for a given word\"\"\"\n        suffixes = self.word_suffixes.get(word, False)\n        if suffixes is not False:\n            return suffixes\n        suffixes = []\n        if word.isalpha():\n            boundary = min(5, len(word))\n            for i in range(1, boundary):\n                suffixes.append(word[-i:])\n        suffixes = tuple(suffixes)\n        self.word_suffixes[word] = suffixes\n        return suffixes\n\n    def getPrefixesForWord(self, word):\n        \"\"\"Generate prefixes for a given word\"\"\"\n        prefixes = self.word_prefixes.get(word, False)\n        if prefixes is not False:\n            return prefixes\n        prefixes = []\n        if word.isalpha():\n            boundary = min(5, len(word))\n            for i in range(2, boundary):\n                prefixes.append(word[:i])\n        prefixes = tuple(prefixes)\n        self.word_prefixes[word] = prefixes\n        return prefixes\n\n    @staticmethod\n    def isNumberWord(word):\n        if word.isdigit():\n            return True\n        elif word.isnumeric():\n            return True\n        elif word.isdecimal():\n            return True\n        else:\n            for char in ('-', ',', '.', '\\/'):\n                word = word.replace(char, '')\n                if word.isdigit():\n                    return True\n            return False\n\n    def getTagSet(self):\n        return tuple(self.tag_dict.keys())\n\n    def getTagSetSize(self):\n        return len(self.tag_dict)\n\n    def getWordDict(self):\n        return self.word_dict\n\n    def getTagDict(self):\n        return self.tag_dict\n\n    def getTagDictSize(self):\n        return len(self.tag_dict)\n\n    def getWordDictSize(self):\n        return len(self.word_dict)\n\n    def getSentences(self):\n        return self.sentences\n\n    def getTags(self):\n        return self.tags\n\n    def getSentencesSize(self):\n        return len(self.sentences)\n\n    def getTagsSize(self):\n        return len(self.tags)\n\n    def getSentenceByIndex(self, index):\n        return self.sentences[index]\n\n    def getTagsByIndex(self, index):\n        return self.tags[index]\n\n    def getWordTagDict(self):\n        return self.word_tag_dict\n\n    def getSortedTagsList(self):\n        return self.sorted_tags_list.copy()\n\n    def getTopNTagsForWord(self, word, n):\n        tags_dict = self.getWordDict().get(word, False)\n        if tags_dict is False:\n            return self.getTopNTags(n)\n        sorted_tags = sorted(tags_dict, key=tags_dict.get, reverse=True)\n        if n == 1:\n            return sorted_tags[0] if sorted_tags else self.sorted_tags_list[0]\n        elif n >= len(sorted_tags):\n            return sorted_tags\n        else:\n            return sorted_tags[:n]\n\n    def getTopNTags(self, n):\n        if n == 1:\n            return self.sorted_tags_list[0]\n        elif n >= len(self.sorted_tags_list):\n            return self.sorted_tags_list\n        else:\n            return self.sorted_tags_list[:n]\n\n    def getNumbers(self):\n        return self.numbers\n\n    def getCapNoStart(self):\n        return self.cap_no_start\n\n    def getCapStart(self):\n        return self.getSentencesSize()\n\nclass VietnameseFeaturesFactory:\n    def __init__(self, data, cutoff=0):\n        self.data = data\n        self.type = \"vietnamese_optimized\"\n        self._cutoff = cutoff\n        self._features_index = {}\n        self.histories_dict = {}\n        self.null_histories_set = set()\n        \n        # Từ đơn âm tiết hay đa âm tiết\n        self.syllable_info = self.__analyze_syllables__()\n        \n        self.__generateFeaturesIndex__()\n\n    def __analyze_syllables__(self):\n        syllable_info = {}\n        for sentence in self.data.getSentences():\n            for word in sentence:\n                # Đếm số âm tiết bằng cách đếm số dấu gạch dưới + 1\n                if '_' in word:\n                    # Từ có dấu gạch dưới: \"bóng_đá\" = 2 âm tiết, \"học_sinh_viên\" = 3 âm tiết\n                    syllable_count = word.count('_') + 1\n                else:\n                    # Từ đơn âm tiết: \"tôi\", \"đẹp\", \"nhà\" = 1 âm tiết\n                    syllable_count = 1\n                syllable_info[word] = syllable_count\n        return syllable_info\n\n    def getSyllablePrefixes(self, word):\n        \"\"\"Return all syllable-level prefixes for a word (joined by '_'). E.g. 'học_sinh_viên' -> ['học', 'học_sinh']\"\"\"\n        if '_' not in word:\n            return []\n        sylls = word.split('_')\n        return ['_'.join(sylls[:i]) for i in range(1, len(sylls))]\n    \n    def getSyllableSuffixes(self, word):\n        \"\"\"Return all syllable-level suffixes for a word (joined by '_'). E.g. 'học_sinh_viên' -> ['viên', 'sinh_viên']\"\"\"\n        if '_' not in word:\n            return []\n        sylls = word.split('_')\n        return ['_'.join(sylls[i:]) for i in range(1, len(sylls))]\n\n    def getFeaturesIndices(self, tag, history, in_data=True):\n        \"\"\"Lấy feature indices tối ưu cho tiếng Việt\"\"\"\n        indices = []\n        word = history.getWord()\n        position = history.getIndex()\n        sentence = history.sentence\n        \n        # 1. f100: (Word,Tag) pair\n        if in_data:\n            feature_idx = self._features_index.get((\"f100\", (word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n\n        # 2. Phonetic (Metaphone) feature\n        metaphone = vn_metaphone(word)\n        feature_idx = self._features_index.get((\"fMetaphone\", (metaphone, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 3. f103: Trigram Tags\n        feature_idx = self._features_index.get((\"f103\", (history.getT_2(), history.getT_1(), tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 4. Bigram Tags\n        feature_idx = self._features_index.get((\"f104\", (history.getT_1(), tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 5. Window features: W_{i-2}, W_{i-1}, W_{i+1}, W_{i+2}\n        for offset in [-2, -1, 1, 2]:\n            idx = position + offset\n            if 0 <= idx < len(sentence):\n                context_word = sentence[idx]\n                feature_idx = self._features_index.get((\"fWindow\", (offset, context_word, tag)), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n        \n        # 6. Word pair features: (W_{i-1}, W_i), (W_i, W_{i+1})\n        if position > 0:\n            prev_word = sentence[position - 1]\n            feature_idx = self._features_index.get((\"fWordPair\", (prev_word, word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        if position < len(sentence) - 1:\n            next_word = sentence[position + 1]\n            feature_idx = self._features_index.get((\"fWordPair\", (word, next_word, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 7. Punctuation feature\n        if tag == 'CH':\n            feature_idx = self._features_index.get((\"fPunct\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 8. Number feature\n        if tag == 'M':\n            feature_idx = self._features_index.get((\"fNum\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 9. Quantifier feature\n        if tag == 'L':\n            feature_idx = self._features_index.get((\"fQuantifier\", tag), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 10. Capitalization features\n        if word and word[0].isupper():\n            if position == 0:\n                feature_idx = self._features_index.get((\"fCapStart\", tag), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n            else:\n                feature_idx = self._features_index.get((\"fCapNoStart\", tag), False)\n                if feature_idx is not False:\n                    indices.append(feature_idx)\n        \n        # 11. Syllable count feature\n        syllable_count = self.syllable_info.get(word, 1)\n        feature_idx = self._features_index.get((\"fSyllable\", (syllable_count, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        # 12. Word length feature\n        length_category = \"short\" if len(word) <= 3 else \"medium\" if len(word) <= 5 else \"long\"\n        feature_idx = self._features_index.get((\"fLength\", (length_category, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n\n        # 13. SyllablePrefix\n        for prefix in self.getSyllablePrefixes(word):\n            feature_idx = self._features_index.get((\"fSyllablePrefix\", (prefix, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 14. SyllableSuffix\n        for suffix in self.getSyllableSuffixes(word):\n            feature_idx = self._features_index.get((\"fSyllableSuffix\", (suffix, tag)), False)\n            if feature_idx is not False:\n                indices.append(feature_idx)\n        \n        # 15. Position in sentence features\n        sent_len = len(sentence)\n        if position == 0:\n            pos_feature = \"first\"\n        elif position == sent_len - 1:\n            pos_feature = \"last\"\n        elif position / sent_len < 0.3:\n            pos_feature = \"early\"\n        elif position / sent_len > 0.7:\n            pos_feature = \"late\"\n        else:\n            pos_feature = \"middle\"\n        \n        feature_idx = self._features_index.get((\"fPosition\", (pos_feature, tag)), False)\n        if feature_idx is not False:\n            indices.append(feature_idx)\n        \n        return indices\n\n    def __generateFeaturesIndex__(self):\n        \"\"\"Generate features index cho tất cả features\"\"\"\n        feature_names = [\n            \"f100\", \"f103\", \"f104\", \"fWindow\", \"fWordPair\", \"fPunct\", \n            \"fNum\", \"fQuantifier\", \"fCapStart\", \"fCapNoStart\", \n            \"fSyllable\", \"fLength\", \"fPosition\",\n            \"fSyllablePrefix\", \"fSyllableSuffix\", \"fMetaphone\"\n        ]\n        \n        # Build feature dictionaries\n        feature_dicts = {}\n        \n        # Existing features\n        feature_dicts[\"f100\"] = self.data.getWordTagDict()\n        feature_dicts[\"f103\"] = self.data.tags_trigrams\n        feature_dicts[\"f104\"] = self.data.tags_bigrams\n        \n        # New features\n        feature_dicts.update(self.__build_new_feature_dicts__())\n        \n        # Generate indices\n        keys = []\n        for name in feature_names:\n            if name in feature_dicts:\n                features = []\n                for feature in feature_dicts[name].keys():\n                    if feature_dicts[name].get(feature) > self._cutoff:\n                        features.append((name, feature))\n                keys.extend(features)\n        \n        for i, key in enumerate(keys):\n            self._features_index[key] = i\n        \n        self.features_list = tuple(keys)\n        self._features_vector_length = len(keys)\n\n    def __build_new_feature_dicts__(self):\n        new_dicts = defaultdict(lambda: defaultdict(int))\n        \n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            \n            for i, (word, tag) in enumerate(zip(sentence, tags)):\n                # Window features\n                for offset in [-2, -1, 1, 2]:\n                    idx = i + offset\n                    if 0 <= idx < len(sentence):\n                        context_word = sentence[idx]\n                        new_dicts[\"fWindow\"][(offset, context_word, tag)] += 1\n\n                metaphone = vn_metaphone(word)\n                new_dicts[\"fMetaphone\"][(metaphone, tag)] += 1\n                \n                # Word pair features\n                if i > 0:\n                    prev_word = sentence[i - 1]\n                    new_dicts[\"fWordPair\"][(prev_word, word, tag)] += 1\n                \n                if i < len(sentence) - 1:\n                    next_word = sentence[i + 1]\n                    new_dicts[\"fWordPair\"][(word, next_word, tag)] += 1\n                \n                # Punctuation feature\n                if tag == 'CH':\n                    new_dicts[\"fPunct\"][tag] += 1\n                \n                # Number feature\n                if tag == 'M':\n                    new_dicts[\"fNum\"][tag] += 1\n                \n                # Quantifier feature\n                if tag == 'L':\n                    new_dicts[\"fQuantifier\"][tag] += 1\n                    \n                # fSyllablePrefix\n                for prefix in self.getSyllablePrefixes(word):\n                    new_dicts[\"fSyllablePrefix\"][(prefix, tag)] += 1\n                \n                # fSyllableSuffix\n                for suffix in self.getSyllableSuffixes(word):\n                    new_dicts[\"fSyllableSuffix\"][(suffix, tag)] += 1\n                \n                # Syllable count\n                syllable_count = self.syllable_info.get(word, 1)\n                new_dicts[\"fSyllable\"][(syllable_count, tag)] += 1\n                \n                # Word length\n                length_category = \"short\" if len(word) <= 3 else \"medium\" if len(word) <= 5 else \"long\"\n                new_dicts[\"fLength\"][(length_category, tag)] += 1\n                \n                # Position in sentence\n                sent_len = len(sentence)\n                if i == 0:\n                    pos_feature = \"first\"\n                elif i == sent_len - 1:\n                    pos_feature = \"last\"\n                elif i / sent_len < 0.3:\n                    pos_feature = \"early\"\n                elif i / sent_len > 0.7:\n                    pos_feature = \"late\"\n                else:\n                    pos_feature = \"middle\"\n                \n                new_dicts[\"fPosition\"][(pos_feature, tag)] += 1\n                \n                # Capitalization features\n                if word and word[0].isupper():\n                    if i == 0:\n                        new_dicts[\"fCapStart\"][tag] += 1\n                    else:\n                        new_dicts[\"fCapNoStart\"][tag] += 1\n        \n        return dict(new_dicts)\n\n    def getFeaturesVectorLength(self):\n        return self._features_vector_length\n    \n    def getCutoffParameter(self):\n        return self._cutoff\n    \n    def getEmpiricalCounts(self):\n        \"\"\"Get empirical counts vector\"\"\"\n        empirical_counts = np.zeros(self.getFeaturesVectorLength(), dtype=float)\n        \n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            \n            for i in range(len(sentence)):\n                history = HistoryTuple(k, sentence, tags, i)\n                features = self.getFeaturesIndices(tags[i], history, True)\n                for feature_idx in features:\n                    empirical_counts[feature_idx] += 1.0\n        \n        return empirical_counts\n\n\nclass ViterbiAlgorithm:\n    \"\"\"Viterbi Algorithm for Vietnamese POS tagging\"\"\"\n    def __init__(self, sequence_id, sentence, sentence_tags, model, cutoff=None):\n        self.sequence_id = sequence_id\n        self.sentence = sentence\n        self.sentence_tags = sentence_tags\n        self.data = model.data\n        if cutoff is None:\n            self.cutoff = self.data.getTagSetSize()\n        else:\n            self.cutoff = cutoff\n        self.tags_set = self.data.getTagSet()\n        self.prob_func = model.probability\n        self.weights = model.getWeights()\n        self.pi = {(-1, BEGIN, BEGIN): 1.0}\n        self.bp = {}\n        self.tag_sequence = []\n\n    def run(self):\n        \"\"\"Main Viterbi algorithm\"\"\"\n        sentence_length = len(self.sentence)\n        for k in range(sentence_length):\n            tag_pairs = tuple(product(self.__calc_possible_tags_set__(k-1), \n                                    self.__calc_possible_tags_set__(k)))\n            for u, v in tag_pairs:\n                key = (k, u, v)\n                self.pi[key], self.bp[k] = self.__calc_max_probability__(key)\n                if self.pi[key] == 0.0000:\n                    self.bp[k] = self.data.getTopNTagsForWord(self.sentence[k], 1)\n        \n        self.bp[sentence_length], self.bp[sentence_length+1] = self.__calc_last_tags__(sentence_length)\n        for k in range(sentence_length):\n            self.tag_sequence.append(self.bp.get(k+2, False))\n\n    def __calc_possible_tags_set__(self, index):\n        \"\"\"Return possible tag set for a given position in the sentence\"\"\"\n        if index < 0:\n            return (BEGIN,)\n        return HistoryTuple(self.sequence_id, self.sentence, self.sentence_tags, index).getPossibleTagSet(self.data, self.cutoff, add_common=False)\n\n    def __calc_max_probability__(self, key):\n        \"\"\"Calculate maximum probability for each iteration\"\"\"\n        k = key[0]\n        u = key[1]\n        v = key[2]\n        if k < 0:\n            return 1.0, BEGIN\n        max_pi = 0.00000\n        max_bp = None\n        possible_tags_set = self.__calc_possible_tags_set__(k-2)\n        for t in possible_tags_set:\n            new_key = (k-1, t, u)\n            history = HistoryTuple(self.sequence_id, self.sentence, self.sentence_tags, k)\n            pi_value = self.pi.get(new_key, 0.00000) * self.prob_func(v, history, self.weights)\n            if pi_value >= max_pi:\n                max_pi = pi_value\n                max_bp = t\n        return max_pi, max_bp\n\n    def __calc_last_tags__(self, sentence_length):\n        \"\"\"Return last 2 tags in the sequence\"\"\"\n        max_pi = 0.0\n        max_bp = ()\n        tag_pairs = tuple(product(self.__calc_possible_tags_set__(sentence_length-2), \n                                self.__calc_possible_tags_set__(sentence_length-1)))\n        for u, v in tag_pairs:\n            key = (sentence_length-1, u, v)\n            pi_value = self.pi.get(key, None)\n            if pi_value and pi_value >= max_pi:\n                max_pi = pi_value\n                max_bp = (u, v)\n        return max_bp\n\n    def getBestTagSequence(self):\n        return tuple(self.tag_sequence)\n\nclass MEMM:\n    \"\"\"MEMM model for Vietnamese POS tagging\"\"\"\n    def __init__(self, feature_factory, regularizer=0, pretrained_weights=None):\n        self.data = feature_factory.data\n        self.feature_factory = feature_factory\n        self.regularizer = float(regularizer)\n        self.cache = self.getTrainedWeightsCacheName()\n        self.weights = self.__initializeWeights__(pretrained_weights)\n        self.train_results = None\n        self.predictions = {}\n        self.correct_tags = defaultdict(int)\n        self.wrong_tags = defaultdict(int)\n        self.wrong_tag_pairs = defaultdict(int)\n        self.wrong_tags_dicts = {}\n\n    def __initializeWeights__(self, pretrained_weights):\n        \"\"\"Initialize model weights\"\"\"\n        weights_vector_length = self.feature_factory.getFeaturesVectorLength()\n        weights = np.zeros(weights_vector_length, dtype=float)\n        if pretrained_weights is True:\n            weights = self.loadTrainedWeights(self.getTrainedWeightsCacheName())\n        elif pretrained_weights is not None and type(pretrained_weights) is np.ndarray and len(pretrained_weights) == weights_vector_length:\n            weights = pretrained_weights\n        return weights\n\n    def getWeights(self):\n        return self.weights\n\n    def getFeatures(self, tag, history, in_data=False):\n        \"\"\"Get feature instances indices for given tag and HistoryTuple\"\"\"\n        history_key = (tag, history.getTupleKey())\n        if history_key in self.feature_factory.null_histories_set:\n            return []\n        feature = self.feature_factory.histories_dict.get(history_key, None)\n        if feature is None:\n            feature = self.feature_factory.getFeaturesIndices(tag, history, in_data)\n            if len(feature) == 0:\n                self.feature_factory.null_histories_set.add(history_key)\n        return feature\n\n    def calc_dot_product(self, features, weights):\n        \"\"\"Calculate dot product between feature and weights vectors\"\"\"\n        total = 0.0\n        for index in features:\n            total += weights[index]\n        return total\n\n    def calcDenominatorBatch(self, history, weights, cutoff=None):\n        \"\"\"Calculate sum in denominator of probability calculation\"\"\"\n        full_tag_set_size = self.data.getTagSetSize()\n        tag_set = history.getPossibleTagSet(self.data, cutoff, add_common=True)\n        remainder = float(full_tag_set_size) - len(tag_set)\n        total = 0.0\n        for tag in tag_set:\n            features = self.getFeatures(tag, history, False)\n            if len(features) == 0:\n                temp = 1.0\n            else:\n                temp = np.exp(self.calc_dot_product(features, weights), dtype=float)\n            total += temp\n        if remainder > 0:\n            total += remainder\n        if total == 0.0:\n            total = 0.0001\n        return total\n\n    def calcNominator(self, features, weights):\n        \"\"\"Calculate nominator in probability calculation\"\"\"\n        if len(features) == 0:\n            nominator = 1.0\n        else:\n            product = self.calc_dot_product(features, weights)\n            if product == 0.0:\n                nominator = 1.0\n            else:\n                nominator = np.exp(product, dtype=float)\n        return nominator\n\n    def probability(self, tag, history, weights, features=None):\n        \"\"\"Calculate probability of specific tag given history\"\"\"\n        if features is None:\n            features = self.getFeatures(tag, history, True)\n        nominator = self.calcNominator(features, weights)\n        denominator = self.calcDenominatorBatch(history, weights)\n        return float(nominator/denominator)\n\n    def calc_loss(self, weights):\n        \"\"\"Calculate loss function value over entire dataset\"\"\"\n        timer = Timer(\"Loss Calculation\")\n        features_sum = 0.0\n        denominators_sum = 0.0\n        for k in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(k)\n            tags = self.data.getTagsByIndex(k)\n            for i in range(len(sentence)):\n                history = HistoryTuple(k, sentence, tags, i)\n                features_sum += self.calc_dot_product(self.getFeatures(tags[i], history, True), weights)\n                denominators_sum += np.log(self.calcDenominatorBatch(history, weights, self.data.getTagSetSize()), dtype=float)\n        if self.regularizer == 1.0:\n            regularization_sum = np.sum(np.power(weights, 2, dtype=float), dtype=float) / 2.0\n        elif self.regularizer != 0.0:\n            regularization_sum = self.regularizer * np.sum(np.power(weights, 2, dtype=float), dtype=float) / 2.0\n        else:\n            regularization_sum = 0.0\n        total = regularization_sum + denominators_sum - features_sum\n        timer.stop()\n        print(\"Loss:\", total)\n        return total\n\n    def calcExpectedCountsDict(self, weights):\n        \"\"\"Calculate expected counts\"\"\"\n        dictionary = defaultdict(float)\n        \n        # Process all sentences sequentially\n        for i in range(self.data.getSentencesSize()):\n            sentence = self.data.getSentenceByIndex(i)\n            tags = self.data.getTagsByIndex(i)\n            for j in range(len(sentence)):\n                history = HistoryTuple(i, sentence, tags, j)\n                self.calcExpectedCountsBatchInternal(history, weights, dictionary)\n        \n        return Counter(dictionary)\n\n    def calcExpectedCountsBatchInternal(self, history, weights, dictionary):\n        \"\"\"Internal function for expected counts calculation\"\"\"\n        cutoff = self.data.getTagSetSize()\n        tag_set = history.getPossibleTagSet(self.data, cutoff, add_common=True)\n        for tag in tag_set:\n            features = self.getFeatures(tag, history, False)\n            if len(features) == 0:\n                continue\n            probability = self.probability(tag, history, weights, features)\n            for index in features:\n                dictionary[index] += probability\n\n    def calc_gradient(self, weights):\n        \"\"\"Calculate gradient vector over entire dataset\"\"\"\n        timer = Timer(\"Gradient Calculation\")\n        empirical_counts = self.feature_factory.getEmpiricalCounts()\n        expected_counts_dict = self.calcExpectedCountsDict(weights)\n        expected_counts = self.calcExpectedCountsVector(expected_counts_dict)\n        if self.regularizer == 1.0:\n            regularization_counts = weights\n        elif self.regularizer != 0.0:\n            regularization_counts = self.regularizer * weights\n        else:\n            regularization_counts = 0.0\n        total = regularization_counts + expected_counts - empirical_counts\n        timer.stop()\n        print(\"Average Gradient value:\", np.mean(total))\n        return total\n\n    def calcExpectedCountsVector(self, dictionary):\n        \"\"\"Convert ExpectedCounts dictionary to numpy vector\"\"\"\n        indexes = dictionary.keys()\n        vector = np.zeros(self.feature_factory.getFeaturesVectorLength(), dtype=float)\n        for index in indexes:\n            vector[index] = dictionary.get(index, 0.0)\n        return vector\n\n    def fit(self, max_iter=100, tolerance=0.001, factr=1e12, save=True):\n        \"\"\"Train the model using L-BFGS-B\"\"\"\n        timer = Timer(\"Training\")\n        weights, loss, result = fmin_l_bfgs_b(self.calc_loss, self.weights, self.calc_gradient, pgtol=tolerance, maxiter=max_iter, factr=factr)\n        if result.get(\"warnflag\", False) != 0:\n            print(\"Warning - gradient didn't converge within\", max_iter, \"iterations\")\n        result['loss'] = loss\n        print(result)\n        self.train_results = result\n        self.weights = weights\n        timer.stop()\n        if save:\n            import os\n            os.makedirs('./cache', exist_ok=True)\n            with open(self.getTrainedWeightsCacheName(), 'wb') as cache:\n                pickle.dump({'weights': self.weights, 'train_results': self.train_results}, cache)\n\n    def predictSequential(self, data, cutoff):\n        \"\"\"Run predictions sequentially instead of using threads\"\"\"\n        timer = Timer(\"Predicting \" + str(data.getSentencesSize()) + \" sentences\")\n        predictions = {}\n        \n        for i in range(data.getSentencesSize()):\n            sentence = data.getSentenceByIndex(i)\n            tags = data.getTagsByIndex(i)\n            viterbi = ViterbiAlgorithm(i, sentence, tags, self, cutoff)\n            viterbi.run()\n            predictions[i] = viterbi.getBestTagSequence()\n        \n        timer.stop()\n        return predictions\n\n    def predict(self, data, cutoff=3):\n        \"\"\"Predict tags for entire dataset\"\"\"\n        timer = Timer(\"Inference\")\n        self.predictions[data.file] = self.predictSequential(data, cutoff)\n        timer.stop()\n\n    def evaluate(self, data, verbose=False):\n        \"\"\"Evaluate model predictions vs truth\"\"\"\n        assert data.getTagsSize() == len(self.predictions.get(data.file, [])), \"Predictions and truth are not the same length!\"\n        timer = Timer(\"Evaluation\")\n        accuracies = []\n        for i in range(data.getTagsSize()):\n            truth = data.getTagsByIndex(i)\n            prediction = self.predictions.get(data.file).get(i, False)\n            accuracies.append(self.accuracy(truth, prediction, verbose))\n        avg = np.mean(accuracies)\n        minimum = np.min(accuracies)\n        maximum = np.max(accuracies)\n        med = np.median(accuracies)\n        print(\"Results for\", data.file)\n        print(\"Total Average Accuracy:\", avg)\n        print(\"Minimal Accuracy:\", minimum)\n        print(\"Maximal Accuracy:\", maximum)\n        print(\"Median Accuracy:\", med)\n        self.confusionTable(data.file)\n        self.confusionMatrix(data.file)\n        timer.stop()\n        return data.file, avg, minimum, maximum, med\n\n    def accuracy(self, truth, predictions, verbose=False):\n        \"\"\"Calculate accuracy for a given sentence\"\"\"\n        assert len(truth) == len(predictions), \"Predictions and truth are not the same length!\"\n        correct = 0\n        for i in range(len(truth)):\n            key = truth[i]\n            subkey = predictions[i]\n            if truth[i] == predictions[i]:\n                correct += 1\n                self.correct_tags[key] += 1\n            else:\n                self.wrong_tags[key] += 1\n                self.wrong_tag_pairs[(key, subkey)] += 1\n                if self.wrong_tags_dicts.get(key, False) is False:\n                    self.wrong_tags_dicts[key] = defaultdict(int)\n                self.wrong_tags_dicts[key][subkey] += 1\n                if verbose:\n                    print(\"Mistake in index\", i, \"(truth, prediction): \", key, subkey)\n        result = float(correct) / len(truth)\n        if verbose:\n            print(\"Accuracy:\", result)\n        return result\n\n    def confusionMatrix(self, file, n=10):\n        \"\"\"Produce Confusion Matrix for top n wrong tags\"\"\"\n        top_wrong_tags = sorted(self.wrong_tags, key=self.wrong_tags.get, reverse=True)[:n]\n        header = top_wrong_tags\n        rows = []\n        for truth in top_wrong_tags:\n            columns = [truth]\n            for prediction in top_wrong_tags:\n                if truth == prediction:\n                    columns.append(self.correct_tags.get(truth))\n                else:\n                    columns.append(self.wrong_tag_pairs.get((truth, prediction)))\n            rows.append(columns)\n        print(\"Confusion Matrix for \" + self.feature_factory.type + \" model on \" + file + \" dataset\")\n        header.insert(0, \"Truth \\ Predicted\")\n        print(tabulate(rows, headers=header))\n\n    def confusionTable(self, file, n=10):\n        \"\"\"Produce Confusion Table for top n wrong tags\"\"\"\n        top_wrong_tags = sorted(self.wrong_tag_pairs, key=self.wrong_tag_pairs.get, reverse=True)[:n]\n        header = (\"Correct Tag\", \"Model's Tag\", \"Frequency\")\n        rows = []\n        for truth, prediction in tuple(top_wrong_tags):\n            freq = self.wrong_tag_pairs.get((truth, prediction))\n            rows.append((truth, prediction, freq))\n        print(\"Confusion Table for \" + self.feature_factory.type + \" model on \" + file + \" dataset\")\n        print(tabulate(rows, headers=header))\n\n    def getTrainedWeightsCacheName(self):\n        \"\"\"Get cache file name according to model parameters\"\"\"\n        prefix = \"./cache/\"\n        parameters = \"data-\" + str(self.data.getSentencesSize()) + \"_features-\" + self.feature_factory.type +\"_weightSize-\"\\\n                     + str(self.feature_factory.getFeaturesVectorLength()) + \"_cutoff-\" + str(self.feature_factory.getCutoffParameter()) \\\n                     + \"_regularizer-\" + str(self.regularizer)\n        suffix = \"_trained_weights.pkl\"\n        return prefix + parameters + suffix\n\n    def loadTrainedWeights(self, file):\n        \"\"\"Load pretrained weights from cache file\"\"\"\n        with open(file, 'rb') as cache:\n            trained = pickle.load(cache)\n            weights = trained.get('weights')\n        return weights\n\n    def evaluate_with_sklearn(self, data, verbose=False):\n        \"\"\"Evaluate model using sklearn metrics\"\"\"\n        assert data.getTagsSize() == len(self.predictions.get(data.file, [])), \"Predictions and truth are not the same length!\"\n        \n        # Flatten all true tags and predictions\n        y_true = []\n        y_pred = []\n        \n        for i in range(data.getTagsSize()):\n            truth = data.getTagsByIndex(i)\n            prediction = self.predictions.get(data.file).get(i, [])\n            \n            y_true.extend(truth)\n            y_pred.extend(prediction)\n        \n        overall_accuracy = accuracy_score(y_true, y_pred)\n        \n        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n        \n        df_report = pd.DataFrame(report).transpose()\n        \n        # Print results\n        print(f\"\\n=== Sklearn Evaluation Results for MEMM ===\")\n        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n        print(\"\\nDetailed Classification Report:\")\n        print(df_report.round(4))\n        \n        # Create a summary table for main metrics\n        main_tags = [tag for tag in df_report.index if tag not in ['accuracy', 'macro avg', 'weighted avg']]\n        summary_data = []\n        \n        for tag in main_tags:\n            summary_data.append({\n                'Tag': tag,\n                'Precision': df_report.loc[tag, 'precision'],\n                'Recall': df_report.loc[tag, 'recall'],\n                'F1-Score': df_report.loc[tag, 'f1-score'],\n                'Support': int(df_report.loc[tag, 'support'])\n            })\n        \n        summary_df = pd.DataFrame(summary_data)\n        \n        # Add overall metrics\n        overall_metrics = pd.DataFrame([\n            {\n                'Tag': 'macro avg',\n                'Precision': df_report.loc['macro avg', 'precision'],\n                'Recall': df_report.loc['macro avg', 'recall'],\n                'F1-Score': df_report.loc['macro avg', 'f1-score'],\n                'Support': int(df_report.loc['macro avg', 'support'])\n            },\n            {\n                'Tag': 'weighted avg',\n                'Precision': df_report.loc['weighted avg', 'precision'],\n                'Recall': df_report.loc['weighted avg', 'recall'],\n                'F1-Score': df_report.loc['weighted avg', 'f1-score'],\n                'Support': int(df_report.loc['weighted avg', 'support'])\n            }\n        ])\n        \n        final_summary = pd.concat([summary_df, overall_metrics], ignore_index=True)\n        \n        print(\"\\n=== Summary Table ===\")\n        print(final_summary.round(4).to_string(index=False))\n        \n        # Show confusion matrix for top tags\n        if verbose:\n            from sklearn.metrics import confusion_matrix\n            import seaborn as sns\n            import matplotlib.pyplot as plt\n            \n            # Get top 10 most frequent tags\n            tag_counts = pd.Series(y_true).value_counts()\n            top_tags = tag_counts.head(10).index.tolist()\n            \n            y_true_filtered = []\n            y_pred_filtered = []\n            \n            for true_tag, pred_tag in zip(y_true, y_pred):\n                if true_tag in top_tags and pred_tag in top_tags:\n                    y_true_filtered.append(true_tag)\n                    y_pred_filtered.append(pred_tag)\n            \n            if len(y_true_filtered) > 0 and len(y_pred_filtered) > 0:\n                cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_tags)\n                \n                plt.figure(figsize=(12, 10))\n                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                           xticklabels=top_tags, yticklabels=top_tags)\n                plt.title('Confusion Matrix - Top 10 Tags')\n                plt.ylabel('True Label')\n                plt.xlabel('Predicted Label')\n                plt.show()\n            else:\n                print(\"Not enough data for confusion matrix visualization\")\n        \n        return {\n            'overall_accuracy': overall_accuracy,\n            'classification_report': report,\n            'summary_df': final_summary,\n            'y_true': y_true,\n            'y_pred': y_pred\n        }\n\nclass Tagger:\n    \n    def __init__(self, model_path, cutoff=3):\n        self.cutoff = cutoff\n        self.memm = self.load_complete_model(model_path)\n        \n        # Use global segmenter instead of loading new one\n        self.segmenter = get_global_segmenter()\n    \n    @staticmethod\n    def load_complete_model(filepath):\n        \"\"\"Load complete model from file\"\"\"\n        with open(filepath, 'rb') as f:\n            model_data = pickle.load(f)\n        \n        # Reconstruct the model\n        model = MEMM(model_data['feature_factory'], \n                    regularizer=model_data['model_metadata']['regularizer'])\n        \n        # Set all the loaded components\n        model.weights = model_data['weights']\n        model.data = model_data['data_reader']\n        model.feature_factory = model_data['feature_factory']\n        model.train_results = model_data.get('train_results')\n        \n        return model\n    \n    def segment_text(self, text):\n        \"\"\"Always segment text using VnCoreNLP\"\"\"\n        if self.segmenter is None:\n            st.error(\"VnCoreNLP not available. Cannot process text.\")\n            return []\n        \n        try:\n            segmented = self.segmenter.word_segment(text)\n            return segmented\n        except Exception as e:\n            st.error(f\"Segmentation error: {str(e)}\")\n            return []\n    \n    def tag_single_word(self, word):\n        \"\"\"Tag a single word using VnCoreNLP segmentation\"\"\"\n        # Always use VnCoreNLP to segment the word (in case it's compound)\n        segmented_sentences = self.segment_text(word)\n        \n        if not segmented_sentences:\n            return 'Unknown'\n        \n        # Take the first segmented sentence\n        segmented_word = segmented_sentences[0].strip()\n        \n        # Split into tokens\n        tokens = segmented_word.split()\n        \n        if not tokens:\n            return 'Unknown'\n        \n        # If it's just one token, add a period for context\n        if len(tokens) == 1:\n            sentence_tokens = [tokens[0], '.']\n        else:\n            sentence_tokens = tokens\n        \n        sentence_tuple = tuple(sentence_tokens)\n        dummy_tags = ['O'] * len(sentence_tuple)\n        \n        # Run Viterbi algorithm\n        viterbi = ViterbiAlgorithm(0, sentence_tuple, dummy_tags, self.memm, cutoff=self.cutoff)\n        viterbi.run()\n        \n        # Return only the tag for the first word\n        tags = list(viterbi.getBestTagSequence())\n        return tags[0] if tags else 'Unknown'\n    \n    def tag_sentence(self, sentence):\n        \"\"\"Tag a sentence using VnCoreNLP segmentation\"\"\"\n        # Always use VnCoreNLP to segment the sentence\n        segmented_sentences = self.segment_text(sentence)\n        \n        if not segmented_sentences:\n            return []\n        \n        # Process each segmented sentence\n        all_results = []\n        for seg_sentence in segmented_sentences:\n            words = seg_sentence.split()\n            if words:  # Only process non-empty sentences\n                tags = self._tag_word_list(words)\n                all_results.append({\n                    'original_input': sentence,\n                    'segmented_sentence': seg_sentence,\n                    'words': words,\n                    'tags': tags\n                })\n        \n        return all_results\n    \n    def _tag_word_list(self, words):\n        \"\"\"Tag a list of words\"\"\"\n        if not words:\n            return []\n            \n        sentence_tuple = tuple(words)\n        dummy_tags = ['O'] * len(sentence_tuple)\n        \n        # Run Viterbi algorithm\n        viterbi = ViterbiAlgorithm(0, sentence_tuple, dummy_tags, self.memm, cutoff=self.cutoff)\n        viterbi.run()\n        \n        return list(viterbi.getBestTagSequence())\n\n\n\nimport re\nimport math\n\ndef load_tagged_sentences(path):\n    \"\"\"Load sentences using CH tag, but only sentence-ending punctuation\"\"\"\n    sentence = []\n    sentence_ending_punctuation = {'.', '!', '?', '...', '..'} \n    \n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue \n                \n            try:\n                word, tag = re.split(r'\\s+', line)\n                sentence.append((word, tag))\n                \n                if tag == 'CH' and word in sentence_ending_punctuation:\n                    if sentence:\n                        yield sentence\n                        sentence = []\n                        \n            except ValueError:\n                # Handle malformed lines\n                continue\n    \n    # Yield final sentence if exists\n    if sentence:\n        yield sentence\n\ndef train_trigram_hmm(sentences_path, min_count=1):\n    # Build vocabulary first\n    word_counts = Counter()\n    for sentence in load_tagged_sentences(sentences_path):\n        for word, _ in sentence:\n            word_counts[word] += 1\n    \n    vocab = {word for word, count in word_counts.items() if count >= min_count}\n    vocab.update(['<UNK>', '<s>', '</s>'])\n    \n    # Train model\n    emission_counts = defaultdict(Counter)\n    trigram_counts = Counter()\n    bigram_counts = Counter()\n    unigram_counts = Counter()\n    tagset = set(['<s>', '</s>'])\n\n    for sentence in load_tagged_sentences(sentences_path):\n        # Replace rare words with <UNK>\n        processed_sentence = []\n        for word, tag in sentence:\n            word = word if word in vocab else '<UNK>'\n            processed_sentence.append((word, tag))\n            tagset.add(tag)\n\n        words = ['<s>', '<s>'] + [w for w, t in processed_sentence] + ['</s>']\n        tags = ['<s>', '<s>'] + [t for w, t in processed_sentence] + ['</s>']\n\n        for word, tag in processed_sentence:\n            emission_counts[tag][word] += 1\n\n        for i in range(len(tags)):\n            unigram_counts[tags[i]] += 1\n        for i in range(len(tags)-1):\n            bigram_counts[(tags[i], tags[i+1])] += 1\n        for i in range(len(tags)-2):\n            trigram_counts[(tags[i], tags[i+1], tags[i+2])] += 1\n\n    return emission_counts, trigram_counts, bigram_counts, unigram_counts, tagset, vocab\n\nclass ProbabilityCache:\n    def __init__(self, emission_counts, trigram_counts, bigram_counts, tagset, vocab):\n        self.emission_counts = emission_counts\n        self.trigram_counts = trigram_counts\n        self.bigram_counts = bigram_counts\n        self.tagset = tagset\n        self.vocab = vocab\n        self.tagset_size = len(tagset)\n        self.vocab_size = len(vocab)\n        \n        # Pre-compute tag totals\n        self.tag_totals = {}\n        for tag in emission_counts:\n            self.tag_totals[tag] = sum(emission_counts[tag].values())\n    \n    def trigram_prob(self, t1, t2, t3):\n        numerator = self.trigram_counts[(t1, t2, t3)] + 1\n        denominator = self.bigram_counts[(t1, t2)] + self.tagset_size\n        return math.log(numerator / denominator)\n    \n    def emission_prob(self, tag, word):\n        numerator = self.emission_counts[tag][word] + 1\n        denominator = self.tag_totals.get(tag, 0) + self.vocab_size\n        return math.log(numerator / denominator)\n\ndef viterbi_decode(sentence, prob_cache):\n    n = len(sentence)\n    tagset = prob_cache.tagset\n    vocab = prob_cache.vocab\n    \n    V = [{} for _ in range(n + 1)]\n    backpointer = [{} for _ in range(n + 1)]\n    \n    V[0][('<s>', '<s>')] = 0.0\n    \n    for i in range(n):\n        word = sentence[i] if sentence[i] in vocab else '<UNK>'\n        \n        for (t1, t2) in V[i]:\n            for t3 in tagset:\n                if t3 in ['<s>', '</s>']:\n                    continue\n                    \n                trans_prob = prob_cache.trigram_prob(t1, t2, t3)\n                emis_prob = prob_cache.emission_prob(t3, word)\n                score = V[i][(t1, t2)] + trans_prob + emis_prob\n                \n                key = (t2, t3)\n                if key not in V[i+1] or score > V[i+1][key]:\n                    V[i+1][key] = score\n                    backpointer[i+1][key] = (t1, t2)\n    \n    if not V[n]:\n        return ['<UNK>'] * n\n    \n    best_final = max(V[n], key=V[n].get)\n    \n    tags = []\n    current = best_final\n    for i in range(n, 0, -1):\n        tags.append(current[1])\n        if i > 1 and current in backpointer[i]:\n            current = backpointer[i][current]\n    \n    return list(reversed(tags))\n\nclass HMM:\n    \"\"\"HMM model for Vietnamese POS tagging\"\"\"\n    def __init__(self, prob_cache=None):\n        self.prob_cache = prob_cache\n        self.train_results = None\n        self.predictions = {}\n        \n    def fit(self, sentences_path, min_count=1, save=True):\n        \"\"\"Train the HMM model\"\"\"\n        timer = Timer(\"HMM Training\")\n        \n        emission_counts, trigram_counts, bigram_counts, unigram_counts, tagset, vocab = train_trigram_hmm(sentences_path, min_count)\n        \n        self.prob_cache = ProbabilityCache(emission_counts, trigram_counts, bigram_counts, tagset, vocab)\n        \n        # Create a simple data reader compatible with MEMM interface\n        self.data = HMMDataReader(tagset, vocab, emission_counts)\n        \n        self.train_results = {\n            'tagset_size': len(tagset),\n            'vocab_size': len(vocab),\n            'emission_features': len(emission_counts),\n            'trigram_features': len(trigram_counts)\n        }\n        \n        timer.stop()\n        \n        if save:\n            self.save_model()\n    \n    def predict(self, data, cutoff=None):\n        \"\"\"Predict tags for sentences\"\"\"\n        timer = Timer(\"HMM Inference\")\n        predictions = {}\n        \n        for i in range(data.getSentencesSize()):\n            sentence = data.getSentenceByIndex(i)\n            predicted_tags = viterbi_decode(sentence, self.prob_cache)\n            predictions[i] = tuple(predicted_tags)\n        \n        self.predictions[data.file] = predictions\n        timer.stop()\n    \n    def tag_sentence_words(self, words):\n        \"\"\"Tag a list of words\"\"\"\n        if not self.prob_cache:\n            return ['<UNK>'] * len(words)\n        \n        return viterbi_decode(words, self.prob_cache)\n    \n    def save_model(self, filepath=None):\n        \"\"\"Save HMM model\"\"\"\n        if filepath is None:\n            filepath = \"./cache/HMM_model.pkl\"\n        \n        save_hmm_model(self.prob_cache, filepath, self.train_results)\n    \n    @staticmethod  # Add this decorator\n    def load_model(model_path):\n        \"\"\"Load a trained HMM model from disk with validation\"\"\"\n        try:\n            with open(model_path, 'rb') as f:\n                model_data = pickle.load(f)\n            \n            # Validate required fields\n            required_fields = ['emission_counts', 'trigram_counts', 'bigram_counts', \n                              'tagset', 'vocab', 'tagset_size', 'vocab_size']\n            \n            for field in required_fields:\n                if field not in model_data:\n                    raise ValueError(f\"Missing required field: {field}\")\n            \n            # Convert back to defaultdict if needed\n            emission_counts = defaultdict(Counter)\n            for tag, word_counts in model_data['emission_counts'].items():\n                emission_counts[tag] = Counter(word_counts)\n            \n            trigram_counts = Counter(model_data['trigram_counts'])\n            bigram_counts = Counter(model_data['bigram_counts'])\n            \n            # Recreate ProbabilityCache with loaded data\n            prob_cache = ProbabilityCache(\n                emission_counts,\n                trigram_counts,\n                bigram_counts,\n                model_data['tagset'],\n                model_data['vocab']\n            )\n            \n            # Create HMM instance\n            hmm = HMM(prob_cache)\n            hmm.train_results = model_data.get('train_results')\n            \n            # Create compatible data reader with error handling\n            try:\n                hmm.data = HMMDataReader(\n                    prob_cache.tagset,\n                    prob_cache.vocab,\n                    prob_cache.emission_counts\n                )\n            except Exception as e:\n                print(f\"Warning: Could not create HMMDataReader: {e}\")\n                # Create minimal data reader as fallback\n                class MinimalHMMData:\n                    def __init__(self, tagset, vocab):\n                        self.tagset = tagset - {'<s>', '</s>'}\n                        self.vocab = vocab\n                        self.file = 'hmm_data'\n                    \n                    def getTagSet(self):\n                        return tuple(self.tagset)\n                    \n                    def getTagSetSize(self):\n                        return len(self.tagset)\n                    \n                    def getWordDictSize(self):\n                        return len(self.vocab)\n                    \n                    def getSentencesSize(self):\n                        return 0\n                    \n                    def getTagsSize(self):\n                        return 0\n                    \n                    def getSentenceByIndex(self, i):\n                        return []\n                    \n                    def getTagsByIndex(self, i):\n                        return []\n                \n                hmm.data = MinimalHMMData(prob_cache.tagset, prob_cache.vocab)\n            \n            print(f\"HMM model loaded from {model_path}\")\n            print(f\"Tagset size: {len(prob_cache.tagset)}\")\n            print(f\"Vocab size: {len(prob_cache.vocab)}\")\n            \n            return hmm\n            \n        except FileNotFoundError:\n            print(f\"Model file not found: {model_path}\")\n            raise\n        except Exception as e:\n            print(f\"Error loading model: {str(e)}\")\n            raise e\n\n# Also add the standalone save/load functions for compatibility:\n\ndef save_hmm_model(prob_cache, model_path, train_results=None):\n    \"\"\"Save the trained HMM model to disk with better error handling\"\"\"\n    try:\n        model_data = {\n            'emission_counts': dict(prob_cache.emission_counts),  # Convert defaultdict to dict\n            'trigram_counts': dict(prob_cache.trigram_counts),\n            'bigram_counts': dict(prob_cache.bigram_counts),\n            'tagset': prob_cache.tagset,\n            'vocab': prob_cache.vocab,\n            'tag_totals': prob_cache.tag_totals,\n            'tagset_size': prob_cache.tagset_size,\n            'vocab_size': prob_cache.vocab_size,\n            'train_results': train_results,\n            'model_type': 'HMM',\n            'version': '1.0'  # Add version for compatibility checking\n        }\n        \n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n        \n        with open(model_path, 'wb') as f:\n            pickle.dump(model_data, f)\n        \n        print(f\"HMM model saved to {model_path}\")\n        print(f\"Tagset size: {len(prob_cache.tagset)}\")\n        print(f\"Vocab size: {len(prob_cache.vocab)}\")\n        \n    except Exception as e:\n        print(f\"Error saving model: {str(e)}\")\n        raise e\n\ndef load_hmm_model(model_path):\n    \"\"\"Load a trained HMM model from disk with validation\"\"\"\n    try:\n        with open(model_path, 'rb') as f:\n            model_data = pickle.load(f)\n        \n        # Validate required fields\n        required_fields = ['emission_counts', 'trigram_counts', 'bigram_counts', \n                          'tagset', 'vocab', 'tagset_size', 'vocab_size']\n        \n        for field in required_fields:\n            if field not in model_data:\n                raise ValueError(f\"Missing required field: {field}\")\n        \n        # Convert back to defaultdict if needed\n        emission_counts = defaultdict(Counter)\n        for tag, word_counts in model_data['emission_counts'].items():\n            emission_counts[tag] = Counter(word_counts)\n        \n        trigram_counts = Counter(model_data['trigram_counts'])\n        bigram_counts = Counter(model_data['bigram_counts'])\n        \n        # Recreate ProbabilityCache with loaded data\n        prob_cache = ProbabilityCache(\n            emission_counts,\n            trigram_counts,\n            bigram_counts,\n            model_data['tagset'],\n            model_data['vocab']\n        )\n        \n        print(f\"HMM model loaded from {model_path}\")\n        print(f\"Tagset size: {len(prob_cache.tagset)}\")\n        print(f\"Vocab size: {len(prob_cache.vocab)}\")\n        \n        return prob_cache, model_data.get('train_results')\n        \n    except FileNotFoundError:\n        print(f\"Model file not found: {model_path}\")\n        raise\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise e\n\nclass HMMDataReader:\n    \"\"\"Data reader compatible with MEMM interface for HMM\"\"\"\n    def __init__(self, tagset, vocab, emission_counts):\n        self.tagset = tagset\n        self.vocab = vocab\n        self.emission_counts = emission_counts\n        self.file = \"hmm_data\"\n        self.sentences = []\n        self.tags = []\n    \n    def getTagSet(self):\n        return tuple(self.tagset - {'<s>', '</s>'})\n    \n    def getTagSetSize(self):\n        return len(self.tagset - {'<s>', '</s>'})\n    \n    def getWordDictSize(self):\n        return len(self.vocab)\n    \n    def getSentencesSize(self):\n        return len(self.sentences)\n    \n    def getTagsSize(self):\n        return len(self.tags)\n    \n    def getSentenceByIndex(self, index):\n        return self.sentences[index]\n    \n    def getTagsByIndex(self, index):\n        return self.tags[index]\n    \n    def set_test_data(self, sentences, tags):\n        \"\"\"Set test data for evaluation\"\"\"\n        self.sentences = sentences\n        self.tags = tags\n\nclass UnifiedTagger:\n    \"\"\"Unified tagger that can work with both MEMM and HMM models\"\"\"\n    \n    def __init__(self, model_path, model_type=\"MEMM\", cutoff=3):\n        self.cutoff = cutoff\n        self.model_type = model_type\n        \n        if model_type == \"MEMM\":\n            self.model = self.load_memm_model(model_path)\n        elif model_type == \"HMM\":\n            self.model = HMM.load_model(model_path)\n        else:\n            raise ValueError(\"model_type must be 'MEMM' or 'HMM'\")\n        \n        # Use global segmenter instead of loading new one\n        self.segmenter = get_global_segmenter()\n    \n    @staticmethod\n    def load_memm_model(filepath):\n        \"\"\"Load MEMM model from file\"\"\"\n        with open(filepath, 'rb') as f:\n            model_data = pickle.load(f)\n        \n        # Reconstruct the model\n        model = MEMM(model_data['feature_factory'], \n                    regularizer=model_data['model_metadata']['regularizer'])\n        \n        # Set all the loaded components\n        model.weights = model_data['weights']\n        model.data = model_data['data_reader']\n        model.feature_factory = model_data['feature_factory']\n        model.train_results = model_data.get('train_results')\n        \n        return model\n    \n    def segment_text(self, text):\n        \"\"\"Always segment text using VnCoreNLP\"\"\"\n        if self.segmenter is None:\n            st.error(\"VnCoreNLP not available. Cannot process text.\")\n            return []\n        \n        try:\n            segmented = self.segmenter.word_segment(text)\n            return segmented\n        except Exception as e:\n            st.error(f\"Segmentation error: {str(e)}\")\n            return []\n    \n    def tag_single_word(self, word):\n        \"\"\"Tag a single word using the selected model\"\"\"\n        # Always use VnCoreNLP to segment the word (in case it's compound)\n        segmented_sentences = self.segment_text(word)\n        \n        if not segmented_sentences:\n            return 'Unknown'\n        \n        # Take the first segmented sentence\n        segmented_word = segmented_sentences[0].strip()\n        \n        # Split into tokens (this preserves underscores in compound words)\n        tokens = segmented_word.split()\n        \n        if not tokens:\n            return 'Unknown'\n        \n        # If it's just one token, add a period for context\n        if len(tokens) == 1:\n            sentence_tokens = [tokens[0], '.']\n        else:\n            sentence_tokens = tokens\n        \n        if self.model_type == \"MEMM\":\n            sentence_tuple = tuple(sentence_tokens)\n            dummy_tags = ['O'] * len(sentence_tuple)\n            \n            # Run Viterbi algorithm\n            viterbi = ViterbiAlgorithm(0, sentence_tuple, dummy_tags, self.model, cutoff=self.cutoff)\n            viterbi.run()\n            \n            # Return only the tag for the first word\n            tags = list(viterbi.getBestTagSequence())\n            return tags[0] if tags else 'Unknown'\n        \n        elif self.model_type == \"HMM\":\n            # Handle unknown words for HMM\n            processed_tokens = []\n            for token in sentence_tokens:\n                if token in self.model.prob_cache.vocab:\n                    processed_tokens.append(token)\n                else:\n                    processed_tokens.append('<UNK>')\n            \n            tags = self.model.tag_sentence_words(processed_tokens)\n            return tags[0] if tags else 'Unknown'\n    \n    def tag_sentence(self, sentence):\n        \"\"\"Tag a sentence using the selected model\"\"\"\n        # Always use VnCoreNLP to segment the sentence\n        segmented_sentences = self.segment_text(sentence)\n        \n        if not segmented_sentences:\n            return []\n        \n        # Process each segmented sentence\n        all_results = []\n        for segmented_sentence in segmented_sentences:\n            # Split the segmented sentence into words (preserving underscores)\n            words = segmented_sentence.split()\n            \n            if words:  # Only process non-empty sentences\n                tags = self._tag_word_list(words)\n                \n                all_results.append({\n                    'original_input': sentence,\n                    'segmented_sentence': segmented_sentence,  # Keep original VnCoreNLP format!\n                    'words': words,\n                    'tags': tags\n                })\n        \n        return all_results\n    \n    def _tag_word_list(self, words):\n        \"\"\"Tag a list of words using the selected model\"\"\"\n        if not words:\n            return []\n        \n        if self.model_type == \"MEMM\":\n            sentence_tuple = tuple(words)\n            dummy_tags = ['O'] * len(sentence_tuple)\n            \n            # Run Viterbi algorithm\n            viterbi = ViterbiAlgorithm(0, sentence_tuple, dummy_tags, self.model, cutoff=self.cutoff)\n            viterbi.run()\n            \n            return list(viterbi.getBestTagSequence())\n        \n        elif self.model_type == \"HMM\":\n            # Handle unknown words for HMM\n            processed_words = []\n            for word in words:\n                if word in self.model.prob_cache.vocab:\n                    processed_words.append(word)\n                else:\n                    processed_words.append('<UNK>')\n            \n            return self.model.tag_sentence_words(processed_words)\n\ndef main():\n    st.set_page_config(\n        page_title=\"Bộ Gán Nhãn Từ Loại Tiếng Việt\",\n        page_icon=\"🏷️\",\n        layout=\"wide\"\n    )\n    \n    st.title(\"🏷️ Bộ Gán Nhãn Từ Loại Tiếng Việt\")\n    st.markdown(\"*Từ Nhóm KKK*\")\n    st.markdown(\"---\")\n    \n    # Initialize VnCoreNLP once at the beginning\n    with st.spinner(\"Đang khởi tạo VnCoreNLP...\"):\n        segmenter = get_global_segmenter()\n        if segmenter is None:\n            st.error(\"❌ Không thể tải VnCoreNLP! Vui lòng kiểm tra cài đặt.\")\n            return\n        else:\n            st.success(\"✅ VnCoreNLP đã được tải thành công!\")\n    \n    # Thanh bên cho việc chọn mô hình\n    st.sidebar.header(\"Cấu Hình Mô Hình\")\n    \n    # Model type selection\n    model_type = st.sidebar.selectbox(\n        \"Chọn Loại Mô Hình:\",\n        options=[\"MEMM\", \"HMM\"],\n        help=\"Chọn loại mô hình: MEMM (Maximum Entropy Markov Model) hoặc HMM (Hidden Markov Model)\"\n    )\n    \n    # Lựa chọn Tập dữ liệu/Mô hình\n    if model_type == \"MEMM\":\n        dataset_options = {\n            \"VNDT\": \"/kaggle/input/nlp_weights/pytorch/default/6/MEMM_VNDT.pkl\",\n            \"Dataset tự tạo\": \"/kaggle/input/nlp_weights/pytorch/default/6/MEMM_custom.pkl\"\n        }\n    else:  # HMM\n        dataset_options = {\n            \"VNDT\": \"/kaggle/input/nlp_weights/pytorch/default/6/HMM_VNDT.pkl\",\n            \"Dataset tự tạo\": \"/kaggle/input/nlp_weights/pytorch/default/6/HMM_custom.pkl\"\n        }\n    \n    selected_dataset = st.sidebar.selectbox(\n        \"Chọn Tập Dữ Liệu/Mô Hình:\",\n        options=list(dataset_options.keys()),\n        help=\"Chọn tập dữ liệu được sử dụng để huấn luyện mô hình\"\n    )\n    \n    # Tham số ngưỡng (only for MEMM)\n    if model_type == \"MEMM\":\n        cutoff = st.sidebar.slider(\n            \"Ngưỡng Gán Nhãn:\",\n            min_value=1,\n            max_value=5,\n            value=1,\n            help=\"Số lượng tối đa các nhãn có thể được xem xét cho mỗi từ\"\n        )\n    else:\n        cutoff = 3  # Default for HMM\n    \n    # Tải mô hình\n    model_path = dataset_options[selected_dataset]\n    \n    try:\n        # Use proper caching for the tagger loading\n        @st.cache_resource\n        def load_tagger_cached(model_path, model_type, cutoff):\n            if model_type == \"MEMM\":\n                return Tagger(model_path, cutoff=cutoff)\n            else:\n                return UnifiedTagger(model_path, model_type=model_type, cutoff=cutoff)\n        \n        with st.spinner(f\"Đang tải mô hình {model_type}...\"):\n            tagger = load_tagger_cached(model_path, model_type, cutoff)\n        \n        st.sidebar.success(f\"✅ Mô hình {model_type} được tải thành công!\")\n        \n        # Get available tags from loaded model\n        if model_type == \"MEMM\":\n            available_tags = list(tagger.memm.data.getTagSet())\n            model_info = {\n                'word_dict_size': tagger.memm.data.getWordDictSize(),\n                'feature_vector_length': tagger.memm.feature_factory.getFeaturesVectorLength(),\n                'additional_info': None\n            }\n        else:\n            available_tags = list(tagger.model.data.getTagSet())\n            model_info = {\n                'word_dict_size': tagger.model.data.getWordDictSize(),\n                'feature_vector_length': None,\n                'additional_info': {\n                    'emission_features': len(tagger.model.prob_cache.emission_counts),\n                    'trigram_features': len(tagger.model.prob_cache.trigram_counts)\n                }\n            }\n        \n        # Display model information\n        with st.sidebar.expander(\"Thông Tin Mô Hình\"):\n            st.write(f\"**Loại Mô Hình:** {model_type}\")\n            st.write(f\"**Tập Dữ Liệu:** {selected_dataset}\")\n            st.write(f\"**Kích Thước Từ Vựng:** {model_info['word_dict_size']:,}\")\n            st.write(f\"**Kích Thước Tập Nhãn:** {len(available_tags)}\")\n            \n            if model_type == \"MEMM\":\n                st.write(f\"**Độ Dài Vector Đặc Trưng:** {model_info['feature_vector_length']:,}\")\n            else:\n                st.write(f\"**Số Đặc Trưng Emission:** {model_info['additional_info']['emission_features']:,}\")\n                st.write(f\"**Số Đặc Trưng Trigram:** {model_info['additional_info']['trigram_features']:,}\")\n            \n            st.write(f\"**Phân Đoạn:** Luôn được bật (VnCoreNLP)\")\n            \n            # Display available tags\n            st.write(f\"**Các Nhãn Có Sẵn:** {', '.join(sorted(available_tags))}\")\n    \n    except Exception as e:\n        st.sidebar.error(f\"❌ Lỗi khi tải mô hình: {str(e)}\")\n        st.error(\"Vui lòng kiểm tra đường dẫn tệp mô hình và thử lại.\")\n        return\n    \n    # Ý nghĩa nhãn động dựa trên các nhãn có sẵn\n    def get_tag_meaning(tag, available_tags):\n        \"\"\"Lấy ý nghĩa nhãn, chỉ khi nhãn tồn tại trong mô hình\"\"\"\n        if tag not in available_tags:\n            return None\n            \n        tag_meanings = {\n            'N': 'Danh từ',\n            'V': 'Động từ', \n            'A': 'Tính từ',\n            'R': 'Phó từ',\n            'E': 'Giới từ',\n            'M': 'Số từ',\n            'L': 'Định từ',\n            'P': 'Đại từ',\n            'C': 'Liên từ',\n            'I': 'Thán từ',\n            'Np': 'Danh từ riêng',\n            'CH': 'Dấu câu',\n            'Nc': 'Danh từ chung',\n            'Nu': 'Danh từ đơn vị',\n            'Ny': 'Danh từ viết tắt',\n            'Cc': 'Liên từ đẳng lập',\n            'T':  'Trợ từ',\n            'X': 'Không xác định',\n            'Y': 'Từ viết tắt',\n            'Vb': 'Động từ cơ bản',\n            'NP': 'Cụm danh từ riêng',\n            'Nb': 'Danh từ mượn',\n            'S': 'Dấu hiệu câu',\n            'WHNP': 'Cụm danh từ nghi vấn'\n        }\n        \n        return tag_meanings.get(tag, f\"Nhãn: {tag}\")\n    \n    def validate_single_word(segmented_text):\n        \"\"\"Xác thực rằng văn bản được phân đoạn là một từ duy nhất (không có khoảng trắng)\"\"\"\n        if ' ' in segmented_text.strip():\n            return False, \"Kết quả phân đoạn chứa nhiều từ. Vui lòng nhập một từ hoặc cụm từ duy nhất.\"\n        return True, None\n    \n    # Khu vực nội dung chính\n    col1, col2 = st.columns([1, 1])\n    \n    with col1:\n        st.header(\"🔤 Gán Nhãn Một Từ\")\n        \n        word_input = st.text_input(\n            \"Nhập một từ tiếng Việt:\",\n            placeholder=\"ví dụ: nhà, đẹp, học, sinh\",\n            help=\"Nhập một từ tiếng Việt duy nhất. Sau khi phân đoạn, nó phải là một từ (có hoặc không có dấu gạch dưới).\"\n        )\n        \n        if st.button(\"Gán Nhãn Từ\", key=\"word_btn\"):\n            if word_input.strip():\n                try:\n                    with st.spinner(\"Đang phân đoạn và gán nhãn...\"):\n                        # Hiển thị kết quả phân đoạn trước\n                        segmented = tagger.segment_text(word_input.strip())\n                        \n                        if segmented:\n                            segmented_text = segmented[0].strip()  # Lấy câu phân đoạn đầu tiên\n                            st.info(f\"**Đã Phân Đoạn:** {segmented_text}\")\n                            \n                            # Xác thực rằng đó là một từ duy nhất\n                            is_valid, error_msg = validate_single_word(segmented_text)\n                            \n                            if not is_valid:\n                                st.error(error_msg)\n                                st.warning(\"Vui lòng thử nhập một từ duy nhất thay vì một cụm từ.\")\n                                return\n                            \n                            # Lấy nhãn bằng cách sử dụng văn bản đã phân đoạn\n                            tag = tagger.tag_single_word(segmented_text)\n                            \n                            st.success(f\"**Đầu Vào Ban Đầu:** {word_input}\")\n                            st.success(f\"**Từ Đã Phân Đoạn:** {segmented_text}\")\n                            st.success(f\"**Nhãn:** {tag}\")\n                            \n                            # Hiển thị ý nghĩa nhãn nếu có\n                            meaning = get_tag_meaning(tag, available_tags)\n                            if meaning:\n                                st.info(f\"**Ý Nghĩa:** {meaning}\")\n                            else:\n                                st.info(f\"**Nhãn:** {tag} (ý nghĩa không có sẵn)\")\n                        else:\n                            st.error(\"Không thể phân đoạn đầu vào.\")\n                        \n                except Exception as e:\n                    st.error(f\"Lỗi khi xử lý đầu vào: {str(e)}\")\n            else:\n                st.warning(\"Vui lòng nhập một từ!\")\n    \n    with col2:\n        st.header(\"📝 Gán Nhãn Văn Bản\")\n        \n        sentence_input = st.text_area(\n            \"Nhập văn bản tiếng Việt:\",\n            placeholder=\"ví dụ: Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan cũng làm việc tại đây.\",\n            height=120,\n            help=\"Nhập văn bản tiếng Việt. Nó sẽ được tự động phân đoạn bằng VnCoreNLP.\"\n        )\n        \n        if st.button(\"Gán Nhãn Văn Bản\", key=\"sentence_btn\"):\n            if sentence_input.strip():\n                try:\n                    with st.spinner(\"Đang phân đoạn và gán nhãn văn bản...\"):\n                        # Tag the sentence directly using the appropriate tagger method\n                        sentence_results = tagger.tag_sentence(sentence_input.strip())\n                    \n                    if sentence_results:\n                        st.success(\"**Kết Quả Xử Lý:**\")\n                        \n                        for i, result in enumerate(sentence_results):\n                            words = result['words']\n                            tags = result['tags']\n                            segmented_sentence = result['segmented_sentence']\n                            \n                            if len(sentence_results) > 1:\n                                st.subheader(f\"Câu {i+1}:\")\n                            \n                            st.write(f\"**Ban Đầu:** {sentence_input}\")\n                            st.write(f\"**Đã Phân Đoạn:** {segmented_sentence}\")\n                            \n                            if len(words) == len(tags):\n                                # Tạo bảng hiển thị đẹp mắt\n                                result_data = []\n                                for j, (word, tag) in enumerate(zip(words, tags)):\n                                    meaning = get_tag_meaning(tag, available_tags)\n                                    result_data.append({\n                                        \"Vị Trí\": j + 1,\n                                        \"Từ\": word,\n                                        \"Nhãn\": tag,\n                                        \"Ý Nghĩa\": meaning if meaning else f\"Nhãn: {tag}\"\n                                    })\n                                \n                                st.table(result_data)\n                                \n                                # Hiển thị các nhãn dưới dạng huy hiệu màu\n                                st.markdown(\"**Biểu Diễn Trực Quan:**\")\n                                \n                                # Gán màu động cho các nhãn có sẵn\n                                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', \n                                         '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43', \n                                         '#FF3838', '#2F3542', '#FF6348', '#7F8C8D', '#9B59B6',\n                                         '#E67E22', '#F39C12', '#D35400', '#C0392B', '#8E44AD']\n                                \n                                tag_colors = {}\n                                for idx, tag in enumerate(sorted(available_tags)):\n                                    tag_colors[tag] = colors[idx % len(colors)]\n                                \n                                html_output = \"\"\n                                for word, tag in zip(words, tags):\n                                    color = tag_colors.get(tag, '#95A5A6')\n                                    html_output += f'<span style=\"background-color: {color}; padding: 4px 8px; margin: 2px; border-radius: 4px; color: white; font-weight: bold;\">{word} <sub>{tag}</sub></span> '\n                                \n                                st.markdown(html_output, unsafe_allow_html=True)\n                                \n                                if len(sentence_results) > 1:\n                                    st.markdown(\"---\")\n                            else:\n                                st.error(\"Số lượng từ và nhãn không khớp!\")\n                    else:\n                        st.error(\"Không thể xử lý văn bản.\")\n                        \n                except Exception as e:\n                    st.error(f\"Lỗi khi xử lý văn bản: {str(e)}\")\n            else:\n                st.warning(\"Vui lòng nhập văn bản!\")\n    \n    # Phần ví dụ\n    st.markdown(\"---\")\n    st.header(\"📚 Ví Dụ\")\n    \n    col3, col4 = st.columns([1, 1])\n    \n    with col3:\n        st.subheader(\"Ví Dụ Gán Nhãn Một Từ\")\n        example_words = [\n            \"nhà\", \n            \"đẹp\", \n            \"học\",\n            \"cơm\",\n            \"đi\",\n            \"100\"\n        ]\n        \n        for word in example_words:\n            if st.button(f\"Gán Nhãn: {word}\", key=f\"example_word_{word}\"):\n                try:\n                    with st.spinner(f\"Đang xử lý {word}...\"):\n                        # Hiển thị phân đoạn\n                        segmented = tagger.segment_text(word)\n                        if segmented:\n                            segmented_text = segmented[0].strip()\n                            st.write(f\"**Ban Đầu:** {word}\")\n                            st.write(f\"**Đã Phân Đoạn:** {segmented_text}\")\n                            \n                            # Xác thực từ đơn\n                            is_valid, error_msg = validate_single_word(segmented_text)\n                            if is_valid:\n                                # Hiển thị nhãn bằng văn bản đã phân đoạn\n                                tag = tagger.tag_single_word(segmented_text)\n                                meaning = get_tag_meaning(tag, available_tags)\n                                st.write(f\"**Nhãn:** {tag}\")\n                                if meaning:\n                                    st.write(f\"**Ý Nghĩa:** {meaning}\")\n                            else:\n                                st.write(f\"**Lỗi:** {error_msg}\")\n                        else:\n                            st.write(f\"Không thể phân đoạn: {word}\")\n                except Exception as e:\n                    st.write(f\"Lỗi khi xử lý {word}: {str(e)}\")\n    \n    with col4:\n        st.subheader(\"Ví Dụ Gán Nhãn Văn Bản\")\n        example_texts = [\n            \"Tôi đang học tiếng Việt.\",\n            \"Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.\",\n            \"Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\",\n            \"Sinh viên làm bài tập ở thư viện.\"\n        ]\n        \n        for i, text in enumerate(example_texts):\n            if st.button(f\"Xử Lý: {text[:30]}...\", key=f\"example_text_{i}\"):\n                try:\n                    with st.spinner(f\"Đang xử lý văn bản {i+1}...\"):\n                        # Use the tag_sentence method directly\n                        results = tagger.tag_sentence(text)\n                        \n                        for result in results:\n                            words = result['words']\n                            tags = result['tags']\n                            \n                            st.write(f\"**Ban Đầu:** {text}\")\n                            st.write(f\"**Đã Phân Đoạn:** {result['segmented_sentence']}\")\n                            \n                            result_str = \" \".join([f\"{w}({t})\" for w, t in zip(words, tags)])\n                            st.write(f\"**Đã Gán Nhãn:** {result_str}\")\n                            st.write(\"---\")\n                except Exception as e:\n                    st.write(f\"Lỗi khi xử lý văn bản: {str(e)}\")\n    \n    # Phần thông tin\n    st.markdown(\"---\")\n    st.header(\"ℹ️ Thông Tin\")\n    \n    with st.expander(\"Về Gán Nhãn Một Từ\"):\n        st.markdown(\"\"\"\n        **Quy Tắc Nhập Từ:**\n        - Đầu vào phải là một từ hoặc cụm từ duy nhất\n        - Sau khi phân đoạn bằng VnCoreNLP, kết quả phải là một token\n        - Ví dụ hợp lệ: \"nhà\" → \"nhà\", \"học sinh\" → \"học_sinh\"\n        - Không hợp lệ: các đầu vào phân đoạn thành nhiều từ cách nhau bằng khoảng trắng\n        \n        **Ví Dụ:**\n        - ✅ \"nhã\" → \"nhà\" (từ đơn)\n        - ✅ \"học sinh\" → \"học_sinh\" (từ ghép có dấu gạch dưới)\n        - ❌ \"tôi đi học\" → \"tôi đi học\" (nhiều từ)\n        \"\"\")\n    \n    with st.expander(\"Về Tích Hợp VnCoreNLP\"):\n        st.markdown(\"\"\"\n        **Phân Đoạn Tự Động:**\n        - Tất cả đầu vào được tự động phân đoạn bằng VnCoreNLP\n        - Các từ ghép được nối đúng cách bằng dấu gạch dưới\n        - Nhiều câu được xử lý riêng biệt\n        \n        **Ví Dụ Phân Đoạn:**\n        - \"học sinh\" → \"học_sinh\"\n        - \"Nguyễn Khắc Chúc\" → \"Nguyễn_Khắc_Chúc\"\n        - \"làm việc\" → \"làm_việc\"\n        - \"Đại học Quốc gia\" → \"Đại_học Quốc_gia\"\n        \"\"\")\n    \n    with st.expander(\"Ý Nghĩa Nhãn Từ Loại\"):\n        # Hiển thị ý nghĩa nhãn động\n        st.markdown(\"**Các Nhãn Có Sẵn Trong Mô Hình Hiện Tại:**\")\n        \n        meanings_text = \"\"\n        for tag in sorted(available_tags):\n            meaning = get_tag_meaning(tag, available_tags)\n            meanings_text += f\"- **{tag}**: {meaning}\\n\"\n        \n        st.markdown(meanings_text)\n    \n    # Chân trang\n    st.markdown(\"---\")\n    st.markdown(\n        \"\"\"\n        <div style='text-align: center'>\n            <p>Bộ Gán Nhãn Từ Loại Tiếng Việt với Phân Đoạn Tự Động VnCoreNLP | Được xây dựng bằng Streamlit</p>\n        </div>\n        \"\"\", \n        unsafe_allow_html=True\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:57:50.327409Z","iopub.execute_input":"2025-07-02T03:57:50.327767Z","iopub.status.idle":"2025-07-02T03:57:50.358125Z","shell.execute_reply.started":"2025-07-02T03:57:50.327740Z","shell.execute_reply":"2025-07-02T03:57:50.357087Z"}},"outputs":[{"name":"stdout","text":"Overwriting app_temp.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!wget -q -O - ipv4.icanhazip.com","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:57:51.844342Z","iopub.execute_input":"2025-07-02T03:57:51.844657Z","iopub.status.idle":"2025-07-02T03:57:51.998451Z","shell.execute_reply.started":"2025-07-02T03:57:51.844634Z","shell.execute_reply":"2025-07-02T03:57:51.997440Z"}},"outputs":[{"name":"stdout","text":"34.23.96.165\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!streamlit run app_temp.py & npx localtunnel --port 8501","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T03:57:52.398840Z","iopub.execute_input":"2025-07-02T03:57:52.399261Z","execution_failed":"2025-07-02T04:31:42.709Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K⠙\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8502\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.23.96.165:8502\u001b[0m\n\u001b[0m\n\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0Kyour url is: https://cute-dragons-beg.loca.lt\n","output_type":"stream"}],"execution_count":null}]}