{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12292790,"sourceType":"datasetVersion","datasetId":7367815}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import defaultdict, Counter\nimport math\nimport re\nimport pickle\nimport os\n\ndef load_tagged_sentences(path):\n    \"\"\"Load sentences using CH tag, but only sentence-ending punctuation\"\"\"\n    sentence = []\n    sentence_ending_punctuation = {'.', '!', '?', '...', '..'}  # Only sentence enders\n    \n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue  # Skip empty lines\n                \n            try:\n                word, tag = re.split(r'\\s+', line)\n                sentence.append((word, tag))\n                \n                # Check if this is end of sentence (CH tag + sentence-ending punctuation)\n                if tag == 'CH' and word in sentence_ending_punctuation:\n                    if sentence:\n                        yield sentence\n                        sentence = []\n                        \n            except ValueError:\n                # Handle malformed lines\n                continue\n    \n    # Yield final sentence if exists\n    if sentence:\n        yield sentence\n\n\n# Step 2: Segment into sentences using 'CH' (punctuation) tag\ndef segment_sentences(flat_tagged_data):\n    sentences = []\n    sentence = []\n    for word, tag in flat_tagged_data:\n        sentence.append((word, tag))\n        if tag == 'CH':\n            sentences.append(sentence)\n            sentence = []\n    if sentence:\n        sentences.append(sentence)\n    return sentences\n\n# Step 3: Train HMM model\ndef train_trigram_hmm(sentences_path, min_count=2):\n    # Build vocabulary first\n    word_counts = Counter()\n    for sentence in load_tagged_sentences(sentences_path):\n        for word, _ in sentence:\n            word_counts[word] += 1\n    \n    vocab = {word for word, count in word_counts.items() if count >= min_count}\n    vocab.update(['<UNK>', '<s>', '</s>'])\n    \n    # Train model\n    emission_counts = defaultdict(Counter)\n    trigram_counts = Counter()\n    bigram_counts = Counter()\n    unigram_counts = Counter()\n    tagset = set(['<s>', '</s>'])\n\n    for sentence in load_tagged_sentences(sentences_path):\n        # Replace rare words with <UNK>\n        processed_sentence = []\n        for word, tag in sentence:\n            word = word if word in vocab else '<UNK>'\n            processed_sentence.append((word, tag))\n            tagset.add(tag)\n\n        words = ['<s>', '<s>'] + [w for w, t in processed_sentence] + ['</s>']\n        tags = ['<s>', '<s>'] + [t for w, t in processed_sentence] + ['</s>']\n\n        for word, tag in processed_sentence:\n            emission_counts[tag][word] += 1\n\n        for i in range(len(tags)):\n            unigram_counts[tags[i]] += 1\n        for i in range(len(tags)-1):\n            bigram_counts[(tags[i], tags[i+1])] += 1\n        for i in range(len(tags)-2):\n            trigram_counts[(tags[i], tags[i+1], tags[i+2])] += 1\n\n    return emission_counts, trigram_counts, bigram_counts, unigram_counts, tagset, vocab\n\n# Add probability cache class\nclass ProbabilityCache:\n    def __init__(self, emission_counts, trigram_counts, bigram_counts, tagset, vocab):\n        self.emission_counts = emission_counts\n        self.trigram_counts = trigram_counts\n        self.bigram_counts = bigram_counts\n        self.tagset = tagset\n        self.vocab = vocab\n        self.tagset_size = len(tagset)\n        self.vocab_size = len(vocab)\n        \n        # Pre-compute tag totals\n        self.tag_totals = {}\n        for tag in emission_counts:\n            self.tag_totals[tag] = sum(emission_counts[tag].values())\n    \n    def trigram_prob(self, t1, t2, t3):\n        numerator = self.trigram_counts[(t1, t2, t3)] + 1\n        denominator = self.bigram_counts[(t1, t2)] + self.tagset_size\n        return math.log(numerator / denominator)\n    \n    def emission_prob(self, tag, word):\n        numerator = self.emission_counts[tag][word] + 1\n        denominator = self.tag_totals.get(tag, 0) + self.vocab_size\n        return math.log(numerator / denominator)\n\n# Add testing function\ndef test_model(test_path, prob_cache):\n    correct = 0\n    total = 0\n    \n    for sentence in load_tagged_sentences(test_path):\n        words = [w for w, _ in sentence]\n        true_tags = [t for _, t in sentence]\n        \n        # Predict tags\n        predicted_tags = viterbi_decode(words, prob_cache)\n        \n        for true_tag, pred_tag in zip(true_tags, predicted_tags):\n            if true_tag == pred_tag:\n                correct += 1\n            total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    return accuracy, correct, total\n\ndef viterbi_decode(sentence, prob_cache):\n    n = len(sentence)\n    tagset = prob_cache.tagset\n    vocab = prob_cache.vocab\n    \n    V = [{} for _ in range(n + 1)]\n    backpointer = [{} for _ in range(n + 1)]\n    \n    V[0][('<s>', '<s>')] = 0.0\n    \n    for i in range(n):\n        word = sentence[i] if sentence[i] in vocab else '<UNK>'\n        \n        for (t1, t2) in V[i]:\n            for t3 in tagset:\n                if t3 in ['<s>', '</s>']:\n                    continue\n                    \n                trans_prob = prob_cache.trigram_prob(t1, t2, t3)\n                emis_prob = prob_cache.emission_prob(t3, word)\n                score = V[i][(t1, t2)] + trans_prob + emis_prob\n                \n                key = (t2, t3)\n                if key not in V[i+1] or score > V[i+1][key]:\n                    V[i+1][key] = score\n                    backpointer[i+1][key] = (t1, t2)\n    \n    if not V[n]:\n        return ['<UNK>'] * n\n    \n    best_final = max(V[n], key=V[n].get)\n    \n    tags = []\n    current = best_final\n    for i in range(n, 0, -1):\n        tags.append(current[1])\n        if i > 1 and current in backpointer[i]:\n            current = backpointer[i][current]\n    \n    return list(reversed(tags))\n\ndef save_hmm_model(prob_cache, model_path, train_results=None):\n    \"\"\"Save the trained HMM model to disk with better error handling\"\"\"\n    try:\n        model_data = {\n            'emission_counts': dict(prob_cache.emission_counts),  # Convert defaultdict to dict\n            'trigram_counts': dict(prob_cache.trigram_counts),\n            'bigram_counts': dict(prob_cache.bigram_counts),\n            'tagset': prob_cache.tagset,\n            'vocab': prob_cache.vocab,\n            'tag_totals': prob_cache.tag_totals,\n            'tagset_size': prob_cache.tagset_size,\n            'vocab_size': prob_cache.vocab_size,\n            'train_results': train_results,\n            'model_type': 'HMM',\n            'version': '1.0'  # Add version for compatibility checking\n        }\n        \n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n        \n        with open(model_path, 'wb') as f:\n            pickle.dump(model_data, f)\n        \n        print(f\"HMM model saved to {model_path}\")\n        print(f\"Tagset size: {len(prob_cache.tagset)}\")\n        print(f\"Vocab size: {len(prob_cache.vocab)}\")\n        \n    except Exception as e:\n        print(f\"Error saving model: {str(e)}\")\n        raise e\n\ndef load_hmm_model(model_path):\n    \"\"\"Load a trained HMM model from disk with validation\"\"\"\n    try:\n        with open(model_path, 'rb') as f:\n            model_data = pickle.load(f)\n        \n        # Validate required fields\n        required_fields = ['emission_counts', 'trigram_counts', 'bigram_counts', \n                          'tagset', 'vocab', 'tagset_size', 'vocab_size']\n        \n        for field in required_fields:\n            if field not in model_data:\n                raise ValueError(f\"Missing required field: {field}\")\n        \n        # Convert back to defaultdict if needed\n        emission_counts = defaultdict(Counter)\n        for tag, word_counts in model_data['emission_counts'].items():\n            emission_counts[tag] = Counter(word_counts)\n        \n        trigram_counts = Counter(model_data['trigram_counts'])\n        bigram_counts = Counter(model_data['bigram_counts'])\n        \n        # Recreate ProbabilityCache with loaded data\n        prob_cache = ProbabilityCache(\n            emission_counts,\n            trigram_counts,\n            bigram_counts,\n            model_data['tagset'],\n            model_data['vocab']\n        )\n        \n        print(f\"HMM model loaded from {model_path}\")\n        print(f\"Tagset size: {len(prob_cache.tagset)}\")\n        print(f\"Vocab size: {len(prob_cache.vocab)}\")\n        \n        return prob_cache, model_data.get('train_results')\n        \n    except FileNotFoundError:\n        print(f\"Model file not found: {model_path}\")\n        raise\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise e","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:22:49.558840Z","iopub.execute_input":"2025-07-01T14:22:49.559061Z","iopub.status.idle":"2025-07-01T14:22:49.593187Z","shell.execute_reply.started":"2025-07-01T14:22:49.559043Z","shell.execute_reply":"2025-07-01T14:22:49.592077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add these imports at the top\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport pandas as pd\nimport numpy as np\n\n# Replace the evaluate_with_sklearn function to work with your code\ndef evaluate_with_sklearn(test_path, prob_cache, verbose=False):\n    \"\"\"Evaluate HMM model using sklearn metrics\"\"\"\n    y_true = []\n    y_pred = []\n    \n    for sentence in load_tagged_sentences(test_path):\n        words = [word for word, _ in sentence]\n        true_tags = [tag for _, tag in sentence]\n        predicted_tags = viterbi_decode(words, prob_cache)\n        \n        # Flatten predictions and true tags\n        y_true.extend(true_tags)\n        y_pred.extend(predicted_tags)\n    \n    # Calculate overall accuracy\n    overall_accuracy = accuracy_score(y_true, y_pred)\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n    \n    # Convert to DataFrame for better visualization\n    df_report = pd.DataFrame(report).transpose()\n    \n    # Print results\n    print(f\"\\n=== Sklearn Evaluation Results for HMM ===\")\n    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n    print(\"\\nDetailed Classification Report:\")\n    print(df_report.round(4))\n    \n    # Create a summary table for main metrics\n    main_tags = [tag for tag in df_report.index if tag not in ['accuracy', 'macro avg', 'weighted avg']]\n    summary_data = []\n    \n    for tag in main_tags:\n        summary_data.append({\n            'Tag': tag,\n            'Precision': df_report.loc[tag, 'precision'],\n            'Recall': df_report.loc[tag, 'recall'],\n            'F1-Score': df_report.loc[tag, 'f1-score'],\n            'Support': int(df_report.loc[tag, 'support'])\n        })\n    \n    summary_df = pd.DataFrame(summary_data)\n    \n    # Add overall metrics\n    overall_metrics = pd.DataFrame([\n        {\n            'Tag': 'macro avg',\n            'Precision': df_report.loc['macro avg', 'precision'],\n            'Recall': df_report.loc['macro avg', 'recall'],\n            'F1-Score': df_report.loc['macro avg', 'f1-score'],\n            'Support': int(df_report.loc['macro avg', 'support'])\n        },\n        {\n            'Tag': 'weighted avg',\n            'Precision': df_report.loc['weighted avg', 'precision'],\n            'Recall': df_report.loc['weighted avg', 'recall'],\n            'F1-Score': df_report.loc['weighted avg', 'f1-score'],\n            'Support': int(df_report.loc['weighted avg', 'support'])\n        }\n    ])\n    \n    final_summary = pd.concat([summary_df, overall_metrics], ignore_index=True)\n    \n    print(\"\\n=== Summary Table ===\")\n    print(final_summary.round(4).to_string(index=False))\n    \n    # Optional: Show confusion matrix for top tags\n    if verbose:\n        try:\n            import seaborn as sns\n            import matplotlib.pyplot as plt\n            \n            # Get top 10 most frequent tags\n            tag_counts = pd.Series(y_true).value_counts()\n            top_tags = tag_counts.head(10).index.tolist()\n            \n            # Filter both arrays simultaneously to maintain alignment\n            y_true_filtered = []\n            y_pred_filtered = []\n            \n            for true_tag, pred_tag in zip(y_true, y_pred):\n                if true_tag in top_tags and pred_tag in top_tags:\n                    y_true_filtered.append(true_tag)\n                    y_pred_filtered.append(pred_tag)\n            \n            # Only create confusion matrix if we have filtered data\n            if len(y_true_filtered) > 0 and len(y_pred_filtered) > 0:\n                cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_tags)\n                \n                plt.figure(figsize=(12, 10))\n                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                           xticklabels=top_tags, yticklabels=top_tags)\n                plt.title('Confusion Matrix - Top 10 Tags (HMM)')\n                plt.ylabel('True Label')\n                plt.xlabel('Predicted Label')\n                plt.show()\n            else:\n                print(\"Not enough data for confusion matrix visualization\")\n        except ImportError:\n            print(\"Matplotlib/Seaborn not available for visualization\")\n    \n    return {\n        'overall_accuracy': overall_accuracy,\n        'classification_report': report,\n        'summary_df': final_summary,\n        'y_true': y_true,\n        'y_pred': y_pred\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:22:49.761654Z","iopub.execute_input":"2025-07-01T14:22:49.761950Z","iopub.status.idle":"2025-07-01T14:22:52.751577Z","shell.execute_reply.started":"2025-07-01T14:22:49.761927Z","shell.execute_reply":"2025-07-01T14:22:52.750627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training model...\")\nemission_counts, trigram_counts, bigram_counts, unigram_counts, tagset, vocab = train_trigram_hmm(\"/kaggle/input/vn-data-pos/train_word_vndt.txt\")\n\n# Create probability cache\nprob_cache = ProbabilityCache(emission_counts, trigram_counts, bigram_counts, tagset, vocab)\n\n# Save the model\nmodel_save_path = \"./models/HMM_custom.pkl\"\nsave_hmm_model(prob_cache, model_save_path)\n\n# Clean up memory\nimport gc\ndel emission_counts, trigram_counts, bigram_counts, unigram_counts\ngc.collect()\n\nprint(\"Training completed and model saved!\")\n\n# Basic evaluation\ntest_accuracy, correct, total = test_model(\"/kaggle/input/vn-data-pos/test_word_vndt.txt\", prob_cache)\nprint(f\"Basic Test Accuracy: {test_accuracy:.4f} ({correct}/{total})\")\n\n# Sklearn evaluation\nprint(\"\\n\" + \"=\"*50)\nprint(\"Evaluating HMM with sklearn...\")\nsklearn_results = evaluate_with_sklearn(\"/kaggle/input/vn-data-pos/test_word_vndt.txt\", prob_cache, verbose=True)\n\n# Comparison table\ncomparison_data = {\n    'Method': ['Basic HMM', 'Sklearn HMM'],\n    'Accuracy': [test_accuracy, sklearn_results['overall_accuracy']],\n    'Model Type': ['Trigram HMM', 'Same model']\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(\"\\n=== Method Comparison ===\")\nprint(comparison_df.to_string(index=False))\n\n# Save results\nsklearn_results['summary_df'].to_csv('hmm_pos_tagging_results.csv', index=False)\nprint(\"HMM results saved to 'hmm_pos_tagging_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:40:00.632501Z","iopub.execute_input":"2025-07-01T14:40:00.633230Z","iopub.status.idle":"2025-07-01T14:40:09.978669Z","shell.execute_reply.started":"2025-07-01T14:40:00.633191Z","shell.execute_reply":"2025-07-01T14:40:09.977407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add these imports at the top\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef visualize_transition_matrix(prob_cache, top_n=15, save_path=None):\n    \"\"\"Visualize trigram transition probabilities as heatmap\"\"\"\n    # Get most common tags (excluding sentence boundaries)\n    regular_tags = [tag for tag in prob_cache.tagset if tag not in ['<s>', '</s>']]\n    tag_counts = {}\n    \n    # Count tag frequencies from bigram counts\n    for (t1, t2) in prob_cache.bigram_counts:\n        if t2 in regular_tags:\n            tag_counts[t2] = tag_counts.get(t2, 0) + prob_cache.bigram_counts[(t1, t2)]\n    \n    # Get top N most frequent tags\n    top_tags = sorted(tag_counts.keys(), key=lambda x: tag_counts.get(x, 0), reverse=True)[:top_n]\n    \n    # Create transition probability matrix for bigrams (simplified from trigrams)\n    transition_matrix = np.zeros((len(top_tags), len(top_tags)))\n    \n    for i, tag1 in enumerate(top_tags):\n        for j, tag2 in enumerate(top_tags):\n            # Use bigram probabilities for visualization\n            numerator = prob_cache.bigram_counts[(tag1, tag2)] + 1\n            denominator = sum(prob_cache.bigram_counts[(tag1, t)] for t in prob_cache.tagset) + len(prob_cache.tagset)\n            transition_matrix[i][j] = numerator / denominator\n    \n    # Create heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(transition_matrix, \n                xticklabels=top_tags, \n                yticklabels=top_tags,\n                annot=True, \n                fmt='.3f', \n                cmap='Blues',\n                cbar_kws={'label': 'Transition Probability'})\n    \n    plt.title(f'Tag Transition Matrix (Top {top_n} Tags)')\n    plt.xlabel('Next Tag')\n    plt.ylabel('Current Tag')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    \n    # Save or show\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"Transition matrix saved to: {save_path}\")\n    else:\n        plt.show()\n    \n    plt.close()  # Close figure to free memory\n    return transition_matrix, top_tags\n\ndef visualize_emission_matrix(prob_cache, top_tags=10, top_words=15, save_path=None):\n    \"\"\"Visualize emission probabilities as heatmap\"\"\"\n    # Get most common tags (excluding sentence boundaries)\n    regular_tags = [tag for tag in prob_cache.tagset if tag not in ['<s>', '</s>']]\n    \n    # Count tag frequencies\n    tag_counts = {}\n    for tag in regular_tags:\n        tag_counts[tag] = prob_cache.tag_totals.get(tag, 0)\n    \n    # Get top N most frequent tags\n    selected_tags = sorted(tag_counts.keys(), key=lambda x: tag_counts.get(x, 0), reverse=True)[:top_tags]\n    \n    # Get most common words for these tags\n    word_counts = {}\n    for tag in selected_tags:\n        for word in prob_cache.emission_counts[tag]:\n            if word != '<UNK>' and word not in ['.', ',', '?', \"!\", ';']:  # Skip unknown token for visualization\n                word_counts[word] = word_counts.get(word, 0) + prob_cache.emission_counts[tag][word]\n    \n    # Get top words\n    top_word_list = sorted(word_counts.keys(), key=lambda x: word_counts.get(x, 0), reverse=True)[:top_words]\n    \n    # Create emission probability matrix\n    emission_matrix = np.zeros((len(selected_tags), len(top_word_list)))\n    \n    for i, tag in enumerate(selected_tags):\n        for j, word in enumerate(top_word_list):\n            numerator = prob_cache.emission_counts[tag][word] + 1\n            denominator = prob_cache.tag_totals.get(tag, 0) + prob_cache.vocab_size\n            emission_matrix[i][j] = numerator / denominator\n    \n    # Create heatmap\n    plt.figure(figsize=(15, 8))\n    sns.heatmap(emission_matrix,\n                xticklabels=top_word_list,\n                yticklabels=selected_tags,\n                annot=True,\n                fmt='.3f',\n                cmap='Blues',\n                cbar_kws={'label': 'Emission Probability'})\n    \n    plt.title(f'Emission Matrix (Top {top_tags} Tags × Top {top_words} Words)')\n    plt.xlabel('Words')\n    plt.ylabel('POS Tags')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    \n    # Save or show\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"Emission matrix saved to: {save_path}\")\n    else:\n        plt.show()\n    \n    plt.close()  # Close figure to free memory\n    return emission_matrix, selected_tags, top_word_list\n\n# Add a convenience function to save both matrices\ndef save_matrices_as_png(prob_cache, output_dir='.', top_n=10):\n    \"\"\"Save both transition and emission matrices as PNG files\"\"\"\n    import os\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define file paths\n    transition_path = os.path.join(output_dir, 'transition_matrix.png')\n    emission_path = os.path.join(output_dir, 'emission_matrix.png')\n    \n    # Save transition matrix\n    print(\"Generating transition matrix...\")\n    visualize_transition_matrix(prob_cache, top_n=top_n, save_path=transition_path)\n    \n    # Save emission matrix\n    print(\"Generating emission matrix...\")\n    visualize_emission_matrix(prob_cache, top_tags=top_n, top_words=top_n, save_path=emission_path)\n    \n    print(f\"\\nMatrices saved in directory: {output_dir}\")\n    return transition_path, emission_path\n\ndef visualize_tag_distribution(prob_cache):\n    \"\"\"Visualize tag frequency distribution\"\"\"\n    # Calculate tag frequencies\n    tag_frequencies = {}\n    for tag in prob_cache.tagset:\n        if tag not in ['<s>', '</s>']:\n            tag_frequencies[tag] = prob_cache.tag_totals.get(tag, 0)\n    \n    # Sort by frequency\n    sorted_tags = sorted(tag_frequencies.items(), key=lambda x: x[1], reverse=True)\n    \n    # Create bar plot\n    plt.figure(figsize=(15, 6))\n    tags, frequencies = zip(*sorted_tags[:20])  # Top 20 tags\n    \n    plt.bar(tags, frequencies, color='skyblue', alpha=0.7)\n    plt.title('POS Tag Frequency Distribution (Top 20)')\n    plt.xlabel('POS Tags')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return sorted_tags\n\ndef analyze_model_statistics(prob_cache):\n    \"\"\"Print detailed model statistics\"\"\"\n    print(\"=== HMM Model Statistics ===\")\n    print(f\"Vocabulary size: {len(prob_cache.vocab):,}\")\n    print(f\"Tagset size: {len(prob_cache.tagset)}\")\n    print(f\"Total trigrams: {len(prob_cache.trigram_counts):,}\")\n    print(f\"Total bigrams: {len(prob_cache.bigram_counts):,}\")\n    \n    # Tag statistics\n    regular_tags = [tag for tag in prob_cache.tagset if tag not in ['<s>', '</s>']]\n    print(f\"Regular tags (excluding boundaries): {len(regular_tags)}\")\n    \n    # Most/least frequent tags\n    tag_frequencies = {tag: prob_cache.tag_totals.get(tag, 0) for tag in regular_tags}\n    most_freq = max(tag_frequencies.items(), key=lambda x: x[1])\n    least_freq = min(tag_frequencies.items(), key=lambda x: x[1])\n    \n    print(f\"Most frequent tag: {most_freq[0]} ({most_freq[1]:,} occurrences)\")\n    print(f\"Least frequent tag: {least_freq[0]} ({least_freq[1]:,} occurrences)\")\n    \n    # Vocabulary statistics\n    word_counts = {}\n    for tag in prob_cache.emission_counts:\n        for word in prob_cache.emission_counts[tag]:\n            word_counts[word] = word_counts.get(word, 0) + prob_cache.emission_counts[tag][word]\n    \n    most_freq_word = max(word_counts.items(), key=lambda x: x[1])\n    print(f\"Most frequent word: '{most_freq_word[0]}' ({most_freq_word[1]:,} occurrences)\")\n    \n    # Sparsity analysis\n    total_possible_trigrams = len(prob_cache.tagset) ** 3\n    trigram_coverage = len(prob_cache.trigram_counts) / total_possible_trigrams\n    print(f\"Trigram coverage: {trigram_coverage:.4f} ({len(prob_cache.trigram_counts):,}/{total_possible_trigrams:,})\")\n\n# Add this function to visualize everything at once\ndef visualize_hmm_model(prob_cache, top_n=5):\n    \"\"\"Comprehensive visualization of HMM model\"\"\"\n    print(\"Generating HMM Model Visualizations...\\n\")\n    \n    # 1. Model statistics\n    analyze_model_statistics(prob_cache)\n    print(\"\\n\" + \"=\"*50)\n    \n    # 2. Tag distribution\n    print(\"1. Tag Frequency Distribution:\")\n    tag_stats = visualize_tag_distribution(prob_cache)\n    \n    # 3. Transition matrix\n    print(\"\\n2. Transition Matrix Visualization:\")\n    transition_matrix, top_tags = visualize_transition_matrix(prob_cache, top_n=top_n)\n    \n    # 4. Emission matrix\n    print(\"\\n3. Emission Matrix Visualization:\")\n    emission_matrix, selected_tags, top_words = visualize_emission_matrix(prob_cache, top_tags=top_n, top_words=top_n)\n    \n    return {\n        'tag_stats': tag_stats,\n        'transition_matrix': transition_matrix,\n        'emission_matrix': emission_matrix,\n        'top_tags': top_tags,\n        'selected_tags': selected_tags,\n        'top_words': top_words\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:24:15.239245Z","iopub.status.idle":"2025-07-01T14:24:15.239743Z","shell.execute_reply.started":"2025-07-01T14:24:15.239535Z","shell.execute_reply":"2025-07-01T14:24:15.239554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage after training your model:\n# Assuming you have prob_cache from training\n\n# Option 1: Save with default settings\ntransition_path, emission_path = save_matrices_as_png(prob_cache)\n\n# Option 2: Save with custom settings\nsave_matrices_as_png(prob_cache, output_dir='./hmm_visualizations', top_n=4)\n\n# Option 3: Save individual matrices with custom paths\nvisualize_transition_matrix(prob_cache, top_n=12, save_path='my_transition_matrix.png')\nvisualize_emission_matrix(prob_cache, top_tags=8, top_words=12, save_path='my_emission_matrix.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:24:15.241181Z","iopub.status.idle":"2025-07-01T14:24:15.241677Z","shell.execute_reply.started":"2025-07-01T14:24:15.241491Z","shell.execute_reply":"2025-07-01T14:24:15.241511Z"}},"outputs":[],"execution_count":null}]}